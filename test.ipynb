{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已经分别移动到 xml 和 png 文件夹。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 假定这是你的数据文件夹路径\n",
    "data_dir = '.'\n",
    "xml_dir = './xml'\n",
    "png_dir = './png'\n",
    "\n",
    "# 创建存放 XML 和 PNG 文件的文件夹\n",
    "os.makedirs(xml_dir, exist_ok=True)\n",
    "os.makedirs(png_dir, exist_ok=True)\n",
    "\n",
    "# 遍历原文件夹，将 XML 和 PNG 文件分别移动到新文件夹\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.xml'):\n",
    "        shutil.move(os.path.join(data_dir, filename), os.path.join(xml_dir, filename))\n",
    "    elif filename.endswith('.png'):\n",
    "        shutil.move(os.path.join(data_dir, filename), os.path.join(png_dir, filename))\n",
    "\n",
    "print('文件已经分别移动到 xml 和 png 文件夹。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxmltodict\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_xml\u001b[39m(xml_file):\n",
      "File \u001b[0;32m~/anaconda3/envs/torchgpu/lib/python3.8/site-packages/torch/__init__.py:234\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# Easy way.  You want this most of the time, because it will prevent\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# C++ symbols from libtorch clobbering C++ symbols from other\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# See Note [Global dependencies]\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[0;32m--> 234\u001b[0m         \u001b[43m_load_global_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchgpu/lib/python3.8/site-packages/torch/__init__.py:174\u001b[0m, in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m lib_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(here), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m'\u001b[39m, lib_name)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRTLD_GLOBAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# Can only happen for wheel with cuda libs as PYPI deps\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# As PyTorch is not purelib, but nvidia-*-cu12 is\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     cuda_libs: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcublas\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibcublas.so.*[0-9]\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcudnn\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibcudnn.so.*[0-9]\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnvtx\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibnvToolsExt.so.*[0-9]\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    190\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/torchgpu/lib/python3.8/ctypes/__init__.py:373\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xmltodict\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def parse_xml(xml_file):\n",
    "    with open(xml_file, 'r') as file:\n",
    "        doc = xmltodict.parse(file.read())\n",
    "    return doc\n",
    "\n",
    "# 遍历 xml 文件夹，解析 XML 文件\n",
    "def prepare_data(xml_dir, png_dir):\n",
    "    dataset = []\n",
    "    for xml_file in os.listdir(xml_dir):\n",
    "        if not xml_file.endswith('.xml'):\n",
    "            continue\n",
    "        \n",
    "        xml_path = os.path.join(xml_dir, xml_file)\n",
    "        png_file = xml_file.replace('.xml', '.png')\n",
    "        png_path = os.path.join(png_dir, png_file)\n",
    "        \n",
    "        # 确保对应的 PNG 文件存在\n",
    "        if not os.path.exists(png_path):\n",
    "            continue\n",
    "        \n",
    "        annotation = parse_xml(xml_path)\n",
    "        \n",
    "        # 修正单个对象和多个对象的处理方式\n",
    "        objects = annotation['annotation']['object']\n",
    "        if not isinstance(objects, list):  # 如果不是列表，将其转换为包含一个元素的列表\n",
    "            objects = [objects]\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        for obj in objects:\n",
    "            bndbox = obj['bndbox']\n",
    "            bboxes.append([int(bndbox['xmin']), int(bndbox['ymin']),\n",
    "                           int(bndbox['xmax']), int(bndbox['ymax'])])\n",
    "            labels.append(1)  # 假设所有对象都是同一个类别\n",
    "        \n",
    "        data_item = {\n",
    "            'image': png_path,\n",
    "            'bboxes': torch.tensor(bboxes, dtype=torch.float32),\n",
    "            'labels': torch.tensor(labels, dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        dataset.append(data_item)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# 准备数据\n",
    "dataset = prepare_data(xml_dir, png_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集划分完成。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "# 设置数据根目录和目标目录\n",
    "data_root = './'\n",
    "target_root = './GRP'\n",
    "\n",
    "# 读取图片和标注文件\n",
    "png_files = glob(os.path.join(data_root, 'png', '*.png'))\n",
    "xml_files = glob(os.path.join(data_root, 'xml', '*.xml'))\n",
    "\n",
    "# 创建文件名到完整路径的映射\n",
    "xml_map = {os.path.basename(os.path.splitext(f)[0]): f for f in xml_files}\n",
    "\n",
    "# 确保每个图片都有对应的XML文件\n",
    "matched_files = [f for f in png_files if os.path.basename(os.path.splitext(f)[0]) in xml_map]\n",
    "\n",
    "# 划分数据集\n",
    "random.shuffle(matched_files)\n",
    "split_point = int(0.7 * len(matched_files))\n",
    "train_files = matched_files[:split_point]\n",
    "test_files = matched_files[split_point:]\n",
    "\n",
    "# 创建目标文件夹\n",
    "for folder in ['train/png', 'train/xml', 'test/png', 'test/xml']:\n",
    "    os.makedirs(os.path.join(target_root, folder), exist_ok=True)\n",
    "\n",
    "# 复制文件到新目录\n",
    "def copy_files(files, folder):\n",
    "    for f in files:\n",
    "        png_dest_path = os.path.join(target_root, folder, os.path.basename(f))\n",
    "        xml_file = xml_map[os.path.basename(os.path.splitext(f)[0])]\n",
    "        xml_dest_path = os.path.join(target_root, folder.replace('png', 'xml'), os.path.basename(xml_file))\n",
    "        try:\n",
    "            shutil.copy(f, png_dest_path)\n",
    "            shutil.copy(xml_file, xml_dest_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying file {os.path.basename(f)} or its XML: {e}\")\n",
    "\n",
    "# 执行复制\n",
    "copy_files(train_files, 'train/png')\n",
    "copy_files(test_files, 'test/png')\n",
    "\n",
    "print(\"数据集划分完成。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 train 中有 1472 张图片和 1472 个标注文件。\n",
      "train 文件夹中的图片和标注文件匹配完整。\n",
      "在 test 中有 632 张图片和 632 个标注文件。\n",
      "test 文件夹中的图片和标注文件匹配完整。\n"
     ]
    }
   ],
   "source": [
    "def verify_dataset(folder):\n",
    "    png_files = glob(os.path.join(target_root, folder, 'png', '*.png'))\n",
    "    xml_files = glob(os.path.join(target_root, folder, 'xml', '*.xml'))\n",
    "\n",
    "    # 检查文件数量\n",
    "    print(f\"在 {folder} 中有 {len(png_files)} 张图片和 {len(xml_files)} 个标注文件。\")\n",
    "\n",
    "    # 检查图片和标注文件的一致性\n",
    "    png_basenames = {os.path.splitext(os.path.basename(f))[0] for f in png_files}\n",
    "    xml_basenames = {os.path.splitext(os.path.basename(f))[0] for f in xml_files}\n",
    "\n",
    "    if png_basenames == xml_basenames:\n",
    "        print(f\"{folder} 文件夹中的图片和标注文件匹配完整。\")\n",
    "    else:\n",
    "        mismatched = png_basenames.symmetric_difference(xml_basenames)\n",
    "        print(f\"{folder} 文件夹中有不匹配的文件：{mismatched}\")\n",
    "\n",
    "# 验证训练集和测试集\n",
    "verify_dataset('train')\n",
    "verify_dataset('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别 'manhole' 有 289 个实例。\n",
      "类别 'cavity' 有 1413 个实例。\n",
      "类别 'loose zone' 有 2056 个实例。\n",
      "类别 'rebar' 有 1188 个实例。\n",
      "类别 'water bearing zone' 有 169 个实例。\n",
      "类别 'concave' 有 3 个实例。\n",
      "类别 'pipeline' 有 20 个实例。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "\n",
    "# 设置数据根目录\n",
    "data_root = './'\n",
    "\n",
    "# 读取所有XML文件\n",
    "xml_files = glob(os.path.join(data_root, 'xml', '*.xml'))\n",
    "\n",
    "# 类别计数器\n",
    "category_counts = Counter()\n",
    "\n",
    "# 遍历每个文件并统计类别\n",
    "for xml_file in xml_files:\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    for obj in root.findall('object'):\n",
    "        category = obj.find('name').text\n",
    "        category_counts[category] += 1\n",
    "\n",
    "# 打印类别及其计数结果\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"类别 '{category}' 有 {count} 个实例。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数量: 1683\n",
      "测试集数量: 421\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(dataset, train_ratio=0.8):\n",
    "    \"\"\"随机打乱数据并分割为训练集和测试集\"\"\"\n",
    "    # 随机打乱索引\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    split = int(len(indices) * train_ratio)\n",
    "    \n",
    "    # 划分训练集和测试集\n",
    "    train_indices = indices[:split]\n",
    "    test_indices = indices[split:]\n",
    "    \n",
    "    # 根据索引划分数据集\n",
    "    train_dataset = [dataset[i] for i in train_indices]\n",
    "    test_dataset = [dataset[i] for i in test_indices]\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# 使用准备好的数据集进行划分\n",
    "train_dataset, test_dataset = split_dataset(dataset)\n",
    "\n",
    "print(f\"训练集数量: {len(train_dataset)}\")\n",
    "print(f\"测试集数量: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = Image.open(item['image']).convert('RGB')\n",
    "        target = {'boxes': item['bboxes'], 'labels': item['labels']}\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "# 数据转换，这里简单地转换为 PyTorch Tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图片转换为 PyTorch Tensor\n",
    "    transforms.Resize((1024, 1024))  # 假设我们需要将图片统一调整到800x800大小\n",
    "])\n",
    "\n",
    "# 创建 PyTorch 数据集\n",
    "train_ds = CustomDataset(train_dataset, transform=transform)\n",
    "test_ds = CustomDataset(test_dataset, transform=transform)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "def calculate_accuracy(tp, fp, fn):\n",
    "    \"\"\"计算准确率\"\"\"\n",
    "    return tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()  # 将模型设置为评估模式\n",
    "    tp = 0  # 真阳性\n",
    "    fp = 0  # 假阳性\n",
    "    fn = 0  # 假阴性\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for i in range(len(images)):\n",
    "                pred_boxes = outputs[i]['boxes'].data.cpu()\n",
    "                pred_labels = outputs[i]['labels'].data.cpu()\n",
    "                pred_scores = outputs[i]['scores'].data.cpu()\n",
    "                \n",
    "                true_boxes = targets[i]['boxes'].data.cpu()\n",
    "                true_labels = targets[i]['labels'].data.cpu()\n",
    "                \n",
    "                if pred_boxes.size(0) == 0 or true_boxes.size(0) == 0:\n",
    "                    fp += pred_boxes.size(0)  # 所有预测都是假阳性，因为没有真实的边界框\n",
    "                    fn += true_boxes.size(0)  # 所有真实的边界框都是假阴性，因为没有预测\n",
    "                    continue\n",
    "                \n",
    "                iou = box_iou(pred_boxes, true_boxes)\n",
    "                \n",
    "                # 选择 IoU > 0.5 作为匹配的框\n",
    "                match_threshold = 0.5\n",
    "                if iou.size(0) > 0 and iou.size(1) > 0:  # 验证是否有可计算的IoU值\n",
    "                    for j, iou_score in enumerate(iou.max(1)[0]):\n",
    "                        if iou_score > match_threshold and pred_labels[j] == true_labels[iou.max(1)[1][j]]:\n",
    "                            tp += 1\n",
    "                        else:\n",
    "                            fp += 1\n",
    "                    fn += (true_boxes.size(0) - iou.max(0)[0].ge(match_threshold).sum().item())\n",
    "\n",
    "    accuracy = calculate_accuracy(tp, fp, fn)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #10 loss: 0.26811137795448303\n",
      "Iteration #20 loss: 0.2633698582649231\n",
      "Iteration #30 loss: 0.2819766700267792\n",
      "Iteration #40 loss: 0.34112468361854553\n",
      "Iteration #50 loss: 0.4944613575935364\n",
      "Iteration #60 loss: 0.3240409791469574\n",
      "Iteration #70 loss: 0.3577042818069458\n",
      "Iteration #80 loss: 0.8500251770019531\n",
      "Iteration #90 loss: 0.37111034989356995\n",
      "Iteration #100 loss: 0.17871831357479095\n",
      "Iteration #110 loss: 0.19433918595314026\n",
      "Iteration #120 loss: 0.6450316309928894\n",
      "Iteration #130 loss: 0.19023685157299042\n",
      "Iteration #140 loss: 0.5023806691169739\n",
      "Iteration #150 loss: 0.4475024342536926\n",
      "Iteration #160 loss: 0.32887524366378784\n",
      "Iteration #170 loss: 0.2788355052471161\n",
      "Iteration #180 loss: 0.4263101816177368\n",
      "Iteration #190 loss: 0.2986884117126465\n",
      "Iteration #200 loss: 0.37839964032173157\n",
      "Iteration #210 loss: 0.37307289242744446\n",
      "Iteration #220 loss: 0.3672666549682617\n",
      "Iteration #230 loss: 0.3832627534866333\n",
      "Iteration #240 loss: 0.3036424219608307\n",
      "Iteration #250 loss: 0.31010740995407104\n",
      "Iteration #260 loss: 0.14421062171459198\n",
      "Iteration #270 loss: 0.42943018674850464\n",
      "Iteration #280 loss: 0.3058638572692871\n",
      "Iteration #290 loss: 0.3905455768108368\n",
      "Iteration #300 loss: 0.2212245762348175\n",
      "Iteration #310 loss: 0.4599163234233856\n",
      "Iteration #320 loss: 0.28375887870788574\n",
      "Iteration #330 loss: 0.359555184841156\n",
      "Iteration #340 loss: 0.4839611351490021\n",
      "Iteration #350 loss: 0.3366490304470062\n",
      "Iteration #360 loss: 0.23718103766441345\n",
      "Iteration #370 loss: 0.3018462359905243\n",
      "Iteration #380 loss: 0.1904306709766388\n",
      "Iteration #390 loss: 0.40424180030822754\n",
      "Iteration #400 loss: 0.28043439984321594\n",
      "Iteration #410 loss: 0.3809281289577484\n",
      "Iteration #420 loss: 0.2547873556613922\n",
      "Epoch #1 average loss: 0.344816093313156\n",
      "Epoch #1 accuracy: 0.0480975211669897\n",
      "Iteration #10 loss: 0.3086354732513428\n",
      "Iteration #20 loss: 0.580687403678894\n",
      "Iteration #30 loss: 0.24525073170661926\n",
      "Iteration #40 loss: 0.3431183397769928\n",
      "Iteration #50 loss: 0.20703043043613434\n",
      "Iteration #60 loss: 0.42850261926651\n",
      "Iteration #70 loss: 0.37987953424453735\n",
      "Iteration #80 loss: 0.23113135993480682\n",
      "Iteration #90 loss: 0.23980171978473663\n",
      "Iteration #100 loss: 0.33754783868789673\n",
      "Iteration #110 loss: 0.35854244232177734\n",
      "Iteration #120 loss: 0.32712891697883606\n",
      "Iteration #130 loss: 0.3616981506347656\n",
      "Iteration #140 loss: 0.5518054962158203\n",
      "Iteration #150 loss: 0.3296895921230316\n",
      "Iteration #160 loss: 0.2879149615764618\n",
      "Iteration #170 loss: 0.38859230279922485\n",
      "Iteration #180 loss: 0.3237791657447815\n",
      "Iteration #190 loss: 0.4343225359916687\n",
      "Iteration #200 loss: 0.4283219575881958\n",
      "Iteration #210 loss: 0.2979757785797119\n",
      "Iteration #220 loss: 0.4667697548866272\n",
      "Iteration #230 loss: 0.22133557498455048\n",
      "Iteration #240 loss: 0.29873013496398926\n",
      "Iteration #250 loss: 0.41200926899909973\n",
      "Iteration #260 loss: 0.29574254155158997\n",
      "Iteration #270 loss: 0.5422605872154236\n",
      "Iteration #280 loss: 0.41626256704330444\n",
      "Iteration #290 loss: 0.31537872552871704\n",
      "Iteration #300 loss: 0.23395873606204987\n",
      "Iteration #310 loss: 0.24834606051445007\n",
      "Iteration #320 loss: 0.3534128665924072\n",
      "Iteration #330 loss: 0.42736801505088806\n",
      "Iteration #340 loss: 0.25128600001335144\n",
      "Iteration #350 loss: 0.2558721899986267\n",
      "Iteration #360 loss: 0.4190475642681122\n",
      "Iteration #370 loss: 0.21626071631908417\n",
      "Iteration #380 loss: 0.19344860315322876\n",
      "Iteration #390 loss: 0.23236700892448425\n",
      "Iteration #400 loss: 0.4634520411491394\n",
      "Iteration #410 loss: 0.4166378676891327\n",
      "Iteration #420 loss: 0.5379476547241211\n",
      "Epoch #2 average loss: 0.3366016462763811\n",
      "Epoch #2 accuracy: 0.0434003015607808\n",
      "Iteration #10 loss: 0.3700031042098999\n",
      "Iteration #20 loss: 0.1568174660205841\n",
      "Iteration #30 loss: 0.2953285574913025\n",
      "Iteration #40 loss: 0.22118476033210754\n",
      "Iteration #50 loss: 0.2873954176902771\n",
      "Iteration #60 loss: 0.09681858122348785\n",
      "Iteration #70 loss: 0.36398401856422424\n",
      "Iteration #80 loss: 0.5835646390914917\n",
      "Iteration #90 loss: 0.5511209964752197\n",
      "Iteration #100 loss: 0.35318127274513245\n",
      "Iteration #110 loss: 0.25283753871917725\n",
      "Iteration #120 loss: 0.393642783164978\n",
      "Iteration #130 loss: 0.35964521765708923\n",
      "Iteration #140 loss: 0.2647520899772644\n",
      "Iteration #150 loss: 0.5954079627990723\n",
      "Iteration #160 loss: 0.2085496038198471\n",
      "Iteration #170 loss: 0.36137115955352783\n",
      "Iteration #180 loss: 0.4202653765678406\n",
      "Iteration #190 loss: 0.2611163854598999\n",
      "Iteration #200 loss: 0.24701108038425446\n",
      "Iteration #210 loss: 0.35218244791030884\n",
      "Iteration #220 loss: 0.3176223635673523\n",
      "Iteration #230 loss: 0.18773740530014038\n",
      "Iteration #240 loss: 0.14548411965370178\n",
      "Iteration #250 loss: 0.5299460291862488\n",
      "Iteration #260 loss: 0.5050476789474487\n",
      "Iteration #270 loss: 0.44038766622543335\n",
      "Iteration #280 loss: 0.23209349811077118\n",
      "Iteration #290 loss: 0.4440237879753113\n",
      "Iteration #300 loss: 0.47155287861824036\n",
      "Iteration #310 loss: 0.25520241260528564\n",
      "Iteration #320 loss: 0.42661982774734497\n",
      "Iteration #330 loss: 0.346038281917572\n",
      "Iteration #340 loss: 0.3331450819969177\n",
      "Iteration #350 loss: 0.23221302032470703\n",
      "Iteration #360 loss: 0.4164321720600128\n",
      "Iteration #370 loss: 0.36964622139930725\n",
      "Iteration #380 loss: 0.22652791440486908\n",
      "Iteration #390 loss: 0.2916858196258545\n",
      "Iteration #400 loss: 0.44195544719696045\n",
      "Iteration #410 loss: 0.17436297237873077\n",
      "Iteration #420 loss: 0.2260095328092575\n",
      "Epoch #3 average loss: 0.3421221882317242\n",
      "Epoch #3 accuracy: 0.05005726283641916\n",
      "Iteration #10 loss: 0.328143447637558\n",
      "Iteration #20 loss: 0.3318173885345459\n",
      "Iteration #30 loss: 0.2947387993335724\n",
      "Iteration #40 loss: 0.19545431435108185\n",
      "Iteration #50 loss: 0.23718689382076263\n",
      "Iteration #60 loss: 0.37638622522354126\n",
      "Iteration #70 loss: 0.22034119069576263\n",
      "Iteration #80 loss: 0.31201231479644775\n",
      "Iteration #90 loss: 0.141410231590271\n",
      "Iteration #100 loss: 0.7133255004882812\n",
      "Iteration #110 loss: 0.6266347169876099\n",
      "Iteration #120 loss: 0.3787367343902588\n",
      "Iteration #130 loss: 0.17787350714206696\n",
      "Iteration #140 loss: 0.4522337317466736\n",
      "Iteration #150 loss: 0.32886338233947754\n",
      "Iteration #160 loss: 0.1703813225030899\n",
      "Iteration #170 loss: 0.29944902658462524\n",
      "Iteration #180 loss: 0.16091547906398773\n",
      "Iteration #190 loss: 0.2805640995502472\n",
      "Iteration #200 loss: 0.39828458428382874\n",
      "Iteration #210 loss: 0.26759132742881775\n",
      "Iteration #220 loss: 0.24494272470474243\n",
      "Iteration #230 loss: 0.3679206371307373\n",
      "Iteration #240 loss: 0.3865114748477936\n",
      "Iteration #250 loss: 0.43451353907585144\n",
      "Iteration #260 loss: 0.2694196403026581\n",
      "Iteration #270 loss: 0.32559922337532043\n",
      "Iteration #280 loss: 0.23886442184448242\n",
      "Iteration #290 loss: 0.3053607940673828\n",
      "Iteration #300 loss: 0.20096924901008606\n",
      "Iteration #310 loss: 0.6110610961914062\n",
      "Iteration #320 loss: 0.2871399521827698\n",
      "Iteration #330 loss: 0.21630991995334625\n",
      "Iteration #340 loss: 0.23199418187141418\n",
      "Iteration #350 loss: 0.4150009751319885\n",
      "Iteration #360 loss: 0.3976854979991913\n",
      "Iteration #370 loss: 0.32810690999031067\n",
      "Iteration #380 loss: 0.26820239424705505\n",
      "Iteration #390 loss: 0.15244820713996887\n",
      "Iteration #400 loss: 0.5084537863731384\n",
      "Iteration #410 loss: 0.2354339361190796\n",
      "Iteration #420 loss: 0.33543509244918823\n",
      "Epoch #4 average loss: 0.33293846095613516\n",
      "Epoch #4 accuracy: 0.04275688458311084\n",
      "Iteration #10 loss: 0.5189646482467651\n",
      "Iteration #20 loss: 0.3213626444339752\n",
      "Iteration #30 loss: 0.3954688310623169\n",
      "Iteration #40 loss: 0.14320683479309082\n",
      "Iteration #50 loss: 0.577099621295929\n",
      "Iteration #60 loss: 0.31218796968460083\n",
      "Iteration #70 loss: 0.3347759544849396\n",
      "Iteration #80 loss: 0.7438299059867859\n",
      "Iteration #90 loss: 0.29853546619415283\n",
      "Iteration #100 loss: 0.5350376963615417\n",
      "Iteration #110 loss: 0.5500395894050598\n",
      "Iteration #120 loss: 0.23613789677619934\n",
      "Iteration #130 loss: 0.3873569071292877\n",
      "Iteration #140 loss: 0.40523484349250793\n",
      "Iteration #150 loss: 0.3218434453010559\n",
      "Iteration #160 loss: 0.7255899906158447\n",
      "Iteration #170 loss: 0.37532341480255127\n",
      "Iteration #180 loss: 0.4300295412540436\n",
      "Iteration #190 loss: 0.16872110962867737\n",
      "Iteration #200 loss: 0.34258395433425903\n",
      "Iteration #210 loss: 0.26825588941574097\n",
      "Iteration #220 loss: 0.2696188688278198\n",
      "Iteration #230 loss: 0.43846145272254944\n",
      "Iteration #240 loss: 0.3946627080440521\n",
      "Iteration #250 loss: 0.47551479935646057\n",
      "Iteration #260 loss: 0.4069419205188751\n",
      "Iteration #270 loss: 0.5174281597137451\n",
      "Iteration #280 loss: 0.3648015856742859\n",
      "Iteration #290 loss: 0.3496723771095276\n",
      "Iteration #300 loss: 0.2677587866783142\n",
      "Iteration #310 loss: 0.40445375442504883\n",
      "Iteration #320 loss: 0.31784653663635254\n",
      "Iteration #330 loss: 0.29426541924476624\n",
      "Iteration #340 loss: 0.4654370844364166\n",
      "Iteration #350 loss: 0.3770199716091156\n",
      "Iteration #360 loss: 0.3030143976211548\n",
      "Iteration #370 loss: 0.351157546043396\n",
      "Iteration #380 loss: 0.339248925447464\n",
      "Iteration #390 loss: 0.37120601534843445\n",
      "Iteration #400 loss: 0.2706541121006012\n",
      "Iteration #410 loss: 0.3585173487663269\n",
      "Iteration #420 loss: 0.3259856700897217\n",
      "Epoch #5 average loss: 0.3350408270141008\n",
      "Epoch #5 accuracy: 0.06411626415900834\n",
      "Iteration #10 loss: 0.3767892122268677\n",
      "Iteration #20 loss: 0.3233458995819092\n",
      "Iteration #30 loss: 0.2440771609544754\n",
      "Iteration #40 loss: 0.45551472902297974\n",
      "Iteration #50 loss: 0.5236601829528809\n",
      "Iteration #60 loss: 0.3212425112724304\n",
      "Iteration #70 loss: 0.18717080354690552\n",
      "Iteration #80 loss: 0.32617127895355225\n",
      "Iteration #90 loss: 0.2914500832557678\n",
      "Iteration #100 loss: 0.27300557494163513\n",
      "Iteration #110 loss: 0.47921210527420044\n",
      "Iteration #120 loss: 0.21792344748973846\n",
      "Iteration #130 loss: 0.3725908398628235\n",
      "Iteration #140 loss: 0.4377857744693756\n",
      "Iteration #150 loss: 0.28232327103614807\n",
      "Iteration #160 loss: 0.2914400100708008\n",
      "Iteration #170 loss: 0.10757722705602646\n",
      "Iteration #180 loss: 0.2572256624698639\n",
      "Iteration #190 loss: 0.45128729939460754\n",
      "Iteration #200 loss: 0.2960861623287201\n",
      "Iteration #210 loss: 0.3925575613975525\n",
      "Iteration #220 loss: 0.38337835669517517\n",
      "Iteration #230 loss: 0.3014034330844879\n",
      "Iteration #240 loss: 0.40067192912101746\n",
      "Iteration #250 loss: 0.447932630777359\n",
      "Iteration #260 loss: 0.11822017282247543\n",
      "Iteration #270 loss: 0.39140236377716064\n",
      "Iteration #280 loss: 0.30003637075424194\n",
      "Iteration #290 loss: 0.240012988448143\n",
      "Iteration #300 loss: 0.4965137243270874\n",
      "Iteration #310 loss: 0.21698299050331116\n",
      "Iteration #320 loss: 0.47156593203544617\n",
      "Iteration #330 loss: 0.4197554290294647\n",
      "Iteration #340 loss: 0.259239137172699\n",
      "Iteration #350 loss: 0.5130605101585388\n",
      "Iteration #360 loss: 0.4181135892868042\n",
      "Iteration #370 loss: 0.4852938652038574\n",
      "Iteration #380 loss: 0.37064328789711\n",
      "Iteration #390 loss: 0.2517360746860504\n",
      "Iteration #400 loss: 0.20975689589977264\n",
      "Iteration #410 loss: 0.24350547790527344\n",
      "Iteration #420 loss: 0.597194492816925\n",
      "Epoch #6 average loss: 0.32986679144044384\n",
      "Epoch #6 accuracy: 0.05193291786999037\n",
      "Iteration #10 loss: 0.3161747455596924\n",
      "Iteration #20 loss: 0.33419403433799744\n",
      "Iteration #30 loss: 0.3632751405239105\n",
      "Iteration #40 loss: 0.25386688113212585\n",
      "Iteration #50 loss: 0.19145995378494263\n",
      "Iteration #60 loss: 0.2623063325881958\n",
      "Iteration #70 loss: 0.447579026222229\n",
      "Iteration #80 loss: 0.34848666191101074\n",
      "Iteration #90 loss: 0.23133714497089386\n",
      "Iteration #100 loss: 0.370649129152298\n",
      "Iteration #110 loss: 0.24666637182235718\n",
      "Iteration #120 loss: 0.38935258984565735\n",
      "Iteration #130 loss: 0.2015245407819748\n",
      "Iteration #140 loss: 0.2672364413738251\n",
      "Iteration #150 loss: 0.30349043011665344\n",
      "Iteration #160 loss: 0.20244218409061432\n",
      "Iteration #170 loss: 0.6015675067901611\n",
      "Iteration #180 loss: 0.5064100027084351\n",
      "Iteration #190 loss: 0.22408996522426605\n",
      "Iteration #200 loss: 0.2618878483772278\n",
      "Iteration #210 loss: 0.32382622361183167\n",
      "Iteration #220 loss: 0.38875511288642883\n",
      "Iteration #230 loss: 0.3312976062297821\n",
      "Iteration #240 loss: 0.3727356791496277\n",
      "Iteration #250 loss: 0.3080769181251526\n",
      "Iteration #260 loss: 0.28086158633232117\n",
      "Iteration #270 loss: 0.21283558011054993\n",
      "Iteration #280 loss: 0.19003283977508545\n",
      "Iteration #290 loss: 0.22757269442081451\n",
      "Iteration #300 loss: 0.48162558674812317\n",
      "Iteration #310 loss: 0.27946600317955017\n",
      "Iteration #320 loss: 0.33795273303985596\n",
      "Iteration #330 loss: 0.3244033455848694\n",
      "Iteration #340 loss: 0.42330437898635864\n",
      "Iteration #350 loss: 0.6094407439231873\n",
      "Iteration #360 loss: 0.35424691438674927\n",
      "Iteration #370 loss: 0.17023120820522308\n",
      "Iteration #380 loss: 0.1999615728855133\n",
      "Iteration #390 loss: 0.39650681614875793\n",
      "Iteration #400 loss: 0.19102619588375092\n",
      "Iteration #410 loss: 0.23268964886665344\n",
      "Iteration #420 loss: 0.1376948356628418\n",
      "Epoch #7 average loss: 0.3241008363504874\n",
      "Epoch #7 accuracy: 0.07298657718120806\n",
      "Iteration #10 loss: 0.16881848871707916\n",
      "Iteration #20 loss: 0.1963680237531662\n",
      "Iteration #30 loss: 0.15189768373966217\n",
      "Iteration #40 loss: 0.17275263369083405\n",
      "Iteration #50 loss: 0.5075991749763489\n",
      "Iteration #60 loss: 0.1514045000076294\n",
      "Iteration #70 loss: 0.11177420616149902\n",
      "Iteration #80 loss: 0.499853253364563\n",
      "Iteration #90 loss: 0.37145671248435974\n",
      "Iteration #100 loss: 0.32455605268478394\n",
      "Iteration #110 loss: 0.20649337768554688\n",
      "Iteration #120 loss: 0.21532320976257324\n",
      "Iteration #130 loss: 0.17355135083198547\n",
      "Iteration #140 loss: 0.22604748606681824\n",
      "Iteration #150 loss: 0.1435472071170807\n",
      "Iteration #160 loss: 0.3108251094818115\n",
      "Iteration #170 loss: 0.19197572767734528\n",
      "Iteration #180 loss: 0.13891972601413727\n",
      "Iteration #190 loss: 0.23091891407966614\n",
      "Iteration #200 loss: 0.1761709302663803\n",
      "Iteration #210 loss: 0.20681746304035187\n",
      "Iteration #220 loss: 0.23030483722686768\n",
      "Iteration #230 loss: 0.4172777533531189\n",
      "Iteration #240 loss: 0.2819991707801819\n",
      "Iteration #250 loss: 0.612962007522583\n",
      "Iteration #260 loss: 0.296221524477005\n",
      "Iteration #270 loss: 0.3599117696285248\n",
      "Iteration #280 loss: 0.2726045548915863\n",
      "Iteration #290 loss: 0.41543877124786377\n",
      "Iteration #300 loss: 0.19926677644252777\n",
      "Iteration #310 loss: 0.3985936939716339\n",
      "Iteration #320 loss: 0.30087023973464966\n",
      "Iteration #330 loss: 0.3166445195674896\n",
      "Iteration #340 loss: 0.1674938201904297\n",
      "Iteration #350 loss: 0.4395824074745178\n",
      "Iteration #360 loss: 0.3595527708530426\n",
      "Iteration #370 loss: 0.26007744669914246\n",
      "Iteration #380 loss: 0.24577990174293518\n",
      "Iteration #390 loss: 0.5003214478492737\n",
      "Iteration #400 loss: 0.36153650283813477\n",
      "Iteration #410 loss: 0.11413876712322235\n",
      "Iteration #420 loss: 0.21468138694763184\n",
      "Epoch #8 average loss: 0.3152730252390236\n",
      "Epoch #8 accuracy: 0.0812984496124031\n",
      "Iteration #10 loss: 0.3688012361526489\n",
      "Iteration #20 loss: 0.22388984262943268\n",
      "Iteration #30 loss: 0.3521232008934021\n",
      "Iteration #40 loss: 0.3787756860256195\n",
      "Iteration #50 loss: 0.42225801944732666\n",
      "Iteration #60 loss: 0.16790741682052612\n",
      "Iteration #70 loss: 0.2164720892906189\n",
      "Iteration #80 loss: 0.29196563363075256\n",
      "Iteration #90 loss: 0.26723185181617737\n",
      "Iteration #100 loss: 0.32179132103919983\n",
      "Iteration #110 loss: 0.16869089007377625\n",
      "Iteration #120 loss: 0.3463130593299866\n",
      "Iteration #130 loss: 0.3295731246471405\n",
      "Iteration #140 loss: 0.2427905946969986\n",
      "Iteration #150 loss: 0.32649803161621094\n",
      "Iteration #160 loss: 0.7520025968551636\n",
      "Iteration #170 loss: 0.2667272388935089\n",
      "Iteration #180 loss: 0.3007155954837799\n",
      "Iteration #190 loss: 0.20648831129074097\n",
      "Iteration #200 loss: 0.40641844272613525\n",
      "Iteration #210 loss: 0.5237519145011902\n",
      "Iteration #220 loss: 0.2306971251964569\n",
      "Iteration #230 loss: 0.17819395661354065\n",
      "Iteration #240 loss: 0.2088472545146942\n",
      "Iteration #250 loss: 0.3743667006492615\n",
      "Iteration #260 loss: 0.2609519958496094\n",
      "Iteration #270 loss: 0.40513160824775696\n",
      "Iteration #280 loss: 0.29073458909988403\n",
      "Iteration #290 loss: 0.3774724006652832\n",
      "Iteration #300 loss: 0.20786575973033905\n",
      "Iteration #310 loss: 0.2289731353521347\n",
      "Iteration #320 loss: 0.357540488243103\n",
      "Iteration #330 loss: 0.23077508807182312\n",
      "Iteration #340 loss: 0.16051508486270905\n",
      "Iteration #350 loss: 0.3416626453399658\n",
      "Iteration #360 loss: 0.2879234850406647\n",
      "Iteration #370 loss: 0.40367376804351807\n",
      "Iteration #380 loss: 0.20686939358711243\n",
      "Iteration #390 loss: 0.4408200681209564\n",
      "Iteration #400 loss: 0.30055150389671326\n",
      "Iteration #410 loss: 0.18103155493736267\n",
      "Iteration #420 loss: 0.13247033953666687\n",
      "Epoch #9 average loss: 0.3060564341957099\n",
      "Epoch #9 accuracy: 0.07040141155712396\n",
      "Iteration #10 loss: 0.3651007413864136\n",
      "Iteration #20 loss: 0.6998868584632874\n",
      "Iteration #30 loss: 0.20727187395095825\n",
      "Iteration #40 loss: 0.22093215584754944\n",
      "Iteration #50 loss: 0.26108312606811523\n",
      "Iteration #60 loss: 0.4311366677284241\n",
      "Iteration #70 loss: 0.17033080756664276\n",
      "Iteration #80 loss: 0.16234345734119415\n",
      "Iteration #90 loss: 0.11464224010705948\n",
      "Iteration #100 loss: 0.336871474981308\n",
      "Iteration #110 loss: 0.13484472036361694\n",
      "Iteration #120 loss: 0.3777071535587311\n",
      "Iteration #130 loss: 0.5284167528152466\n",
      "Iteration #140 loss: 0.13446739315986633\n",
      "Iteration #150 loss: 0.18775902688503265\n",
      "Iteration #160 loss: 0.39463189244270325\n",
      "Iteration #170 loss: 0.34107643365859985\n",
      "Iteration #180 loss: 0.43778374791145325\n",
      "Iteration #190 loss: 0.19058562815189362\n",
      "Iteration #200 loss: 0.19233818352222443\n",
      "Iteration #210 loss: 0.5645307302474976\n",
      "Iteration #220 loss: 0.35613375902175903\n",
      "Iteration #230 loss: 0.23916570842266083\n",
      "Iteration #240 loss: 0.18674041330814362\n",
      "Iteration #250 loss: 0.10029121488332748\n",
      "Iteration #260 loss: 0.16202472150325775\n",
      "Iteration #270 loss: 0.49611595273017883\n",
      "Iteration #280 loss: 0.31760865449905396\n",
      "Iteration #290 loss: 0.23110496997833252\n",
      "Iteration #300 loss: 0.1589796245098114\n",
      "Iteration #310 loss: 0.2415887415409088\n",
      "Iteration #320 loss: 0.2360471487045288\n",
      "Iteration #330 loss: 0.15927943587303162\n",
      "Iteration #340 loss: 0.21781477332115173\n",
      "Iteration #350 loss: 0.24516808986663818\n",
      "Iteration #360 loss: 0.2577632963657379\n",
      "Iteration #370 loss: 0.4022689759731293\n",
      "Iteration #380 loss: 0.5486178994178772\n",
      "Iteration #390 loss: 0.24895000457763672\n",
      "Iteration #400 loss: 0.3289044499397278\n",
      "Iteration #410 loss: 0.46251577138900757\n",
      "Iteration #420 loss: 0.470504492521286\n",
      "Epoch #10 average loss: 0.2950473690931984\n",
      "Epoch #10 accuracy: 0.08920014743826023\n",
      "Iteration #10 loss: 0.08388727903366089\n",
      "Iteration #20 loss: 0.2749951183795929\n",
      "Iteration #30 loss: 0.11019980907440186\n",
      "Iteration #40 loss: 0.3586050271987915\n",
      "Iteration #50 loss: 0.2984095513820648\n",
      "Iteration #60 loss: 0.3404507040977478\n",
      "Iteration #70 loss: 0.2156069129705429\n",
      "Iteration #80 loss: 0.2195136994123459\n",
      "Iteration #90 loss: 0.17240622639656067\n",
      "Iteration #100 loss: 0.7166070342063904\n",
      "Iteration #110 loss: 0.3103350102901459\n",
      "Iteration #120 loss: 0.37494730949401855\n",
      "Iteration #130 loss: 0.18360385298728943\n",
      "Iteration #140 loss: 0.30400341749191284\n",
      "Iteration #150 loss: 0.10223525762557983\n",
      "Iteration #160 loss: 0.15261316299438477\n",
      "Iteration #170 loss: 0.38364359736442566\n",
      "Iteration #180 loss: 0.32065650820732117\n",
      "Iteration #190 loss: 0.21344268321990967\n",
      "Iteration #200 loss: 0.19675561785697937\n",
      "Iteration #210 loss: 0.22435319423675537\n",
      "Iteration #220 loss: 0.3942365348339081\n",
      "Iteration #230 loss: 0.1441417783498764\n",
      "Iteration #240 loss: 0.2571306526660919\n",
      "Iteration #250 loss: 0.12166837602853775\n",
      "Iteration #260 loss: 0.23269227147102356\n",
      "Iteration #270 loss: 0.2944858968257904\n",
      "Iteration #280 loss: 0.316141813993454\n",
      "Iteration #290 loss: 0.23966072499752045\n",
      "Iteration #300 loss: 0.2549036741256714\n",
      "Iteration #310 loss: 0.5741243958473206\n",
      "Iteration #320 loss: 0.40543168783187866\n",
      "Iteration #330 loss: 0.47647133469581604\n",
      "Iteration #340 loss: 0.40049076080322266\n",
      "Iteration #350 loss: 0.12017166614532471\n",
      "Iteration #360 loss: 0.4149903655052185\n",
      "Iteration #370 loss: 0.2907405197620392\n",
      "Iteration #380 loss: 0.212543323636055\n",
      "Iteration #390 loss: 0.16708511114120483\n",
      "Iteration #400 loss: 0.3315299451351166\n",
      "Iteration #410 loss: 0.1820307970046997\n",
      "Iteration #420 loss: 0.31396329402923584\n",
      "Epoch #11 average loss: 0.2794027194522339\n",
      "Epoch #11 accuracy: 0.07794329649207112\n",
      "Iteration #10 loss: 0.2853533625602722\n",
      "Iteration #20 loss: 0.30969470739364624\n",
      "Iteration #30 loss: 0.17899441719055176\n",
      "Iteration #40 loss: 0.23549069464206696\n",
      "Iteration #50 loss: 0.4172850251197815\n",
      "Iteration #60 loss: 0.19089432060718536\n",
      "Iteration #70 loss: 0.3450912535190582\n",
      "Iteration #80 loss: 0.1833759993314743\n",
      "Iteration #90 loss: 0.16552889347076416\n",
      "Iteration #100 loss: 0.38710880279541016\n",
      "Iteration #110 loss: 0.14228495955467224\n",
      "Iteration #120 loss: 0.5075317621231079\n",
      "Iteration #130 loss: 0.1549968123435974\n",
      "Iteration #140 loss: 0.20163293182849884\n",
      "Iteration #150 loss: 0.5424038767814636\n",
      "Iteration #160 loss: 0.22526083886623383\n",
      "Iteration #170 loss: 0.3391796946525574\n",
      "Iteration #180 loss: 0.1535174697637558\n",
      "Iteration #190 loss: 0.1785699427127838\n",
      "Iteration #200 loss: 0.33956533670425415\n",
      "Iteration #210 loss: 0.17326433956623077\n",
      "Iteration #220 loss: 0.1454898566007614\n",
      "Iteration #230 loss: 0.4299568235874176\n",
      "Iteration #240 loss: 0.19640667736530304\n",
      "Iteration #250 loss: 0.4312851130962372\n",
      "Iteration #260 loss: 0.4599147140979767\n",
      "Iteration #270 loss: 0.236862912774086\n",
      "Iteration #280 loss: 0.15130352973937988\n",
      "Iteration #290 loss: 0.338494211435318\n",
      "Iteration #300 loss: 0.18751513957977295\n",
      "Iteration #310 loss: 0.4668877124786377\n",
      "Iteration #320 loss: 0.24516375362873077\n",
      "Iteration #330 loss: 0.24136540293693542\n",
      "Iteration #340 loss: 0.20649389922618866\n",
      "Iteration #350 loss: 0.39033254981040955\n",
      "Iteration #360 loss: 0.25640565156936646\n",
      "Iteration #370 loss: 0.2668086290359497\n",
      "Iteration #380 loss: 0.339194655418396\n",
      "Iteration #390 loss: 0.3943963050842285\n",
      "Iteration #400 loss: 0.1572173684835434\n",
      "Iteration #410 loss: 0.28646326065063477\n",
      "Iteration #420 loss: 0.1290559023618698\n",
      "Epoch #12 average loss: 0.2706441494178319\n",
      "Epoch #12 accuracy: 0.07013058168203817\n",
      "Iteration #10 loss: 0.17894503474235535\n",
      "Iteration #20 loss: 0.28424352407455444\n",
      "Iteration #30 loss: 0.24080507457256317\n",
      "Iteration #40 loss: 0.308206707239151\n",
      "Iteration #50 loss: 0.29676204919815063\n",
      "Iteration #60 loss: 0.38912123441696167\n",
      "Iteration #70 loss: 0.44024351239204407\n",
      "Iteration #80 loss: 0.16059812903404236\n",
      "Iteration #90 loss: 0.13563677668571472\n",
      "Iteration #100 loss: 0.27201294898986816\n",
      "Iteration #110 loss: 0.29366639256477356\n",
      "Iteration #120 loss: 0.2651141881942749\n",
      "Iteration #130 loss: 0.41031140089035034\n",
      "Iteration #140 loss: 0.2071227878332138\n",
      "Iteration #150 loss: 0.42238208651542664\n",
      "Iteration #160 loss: 0.24696610867977142\n",
      "Iteration #170 loss: 0.11060138791799545\n",
      "Iteration #180 loss: 0.26417863368988037\n",
      "Iteration #190 loss: 0.318250447511673\n",
      "Iteration #200 loss: 0.12834998965263367\n",
      "Iteration #210 loss: 0.4246312379837036\n",
      "Iteration #220 loss: 0.5657262802124023\n",
      "Iteration #230 loss: 0.2170208990573883\n",
      "Iteration #240 loss: 0.1216241866350174\n",
      "Iteration #250 loss: 0.16486839950084686\n",
      "Iteration #260 loss: 0.37882480025291443\n",
      "Iteration #270 loss: 0.2940327823162079\n",
      "Iteration #280 loss: 0.2527614235877991\n",
      "Iteration #290 loss: 0.35380977392196655\n",
      "Iteration #300 loss: 0.40932363271713257\n",
      "Iteration #310 loss: 0.31527262926101685\n",
      "Iteration #320 loss: 0.18862204253673553\n",
      "Iteration #330 loss: 0.21395954489707947\n",
      "Iteration #340 loss: 0.2834344506263733\n",
      "Iteration #350 loss: 0.22959542274475098\n",
      "Iteration #360 loss: 0.3478561341762543\n",
      "Iteration #370 loss: 0.1761315017938614\n",
      "Iteration #380 loss: 0.3468692898750305\n",
      "Iteration #390 loss: 0.2032458782196045\n",
      "Iteration #400 loss: 0.09156861901283264\n",
      "Iteration #410 loss: 0.27977287769317627\n",
      "Iteration #420 loss: 0.1989111751317978\n",
      "Epoch #13 average loss: 0.2597205980444181\n",
      "Epoch #13 accuracy: 0.07867149086186102\n",
      "Iteration #10 loss: 0.3545815050601959\n",
      "Iteration #20 loss: 0.30507218837738037\n",
      "Iteration #30 loss: 0.27588576078414917\n",
      "Iteration #40 loss: 0.20725151896476746\n",
      "Iteration #50 loss: 0.31204062700271606\n",
      "Iteration #60 loss: 0.3870987594127655\n",
      "Iteration #70 loss: 0.14733164012432098\n",
      "Iteration #80 loss: 0.10405347496271133\n",
      "Iteration #90 loss: 0.07252385467290878\n",
      "Iteration #100 loss: 0.1464909166097641\n",
      "Iteration #110 loss: 0.19596685469150543\n",
      "Iteration #120 loss: 0.17897523939609528\n",
      "Iteration #130 loss: 0.12970757484436035\n",
      "Iteration #140 loss: 0.4203783869743347\n",
      "Iteration #150 loss: 0.23252619802951813\n",
      "Iteration #160 loss: 0.25323858857154846\n",
      "Iteration #170 loss: 0.5035954713821411\n",
      "Iteration #180 loss: 0.17285770177841187\n",
      "Iteration #190 loss: 0.3482400178909302\n",
      "Iteration #200 loss: 0.2672954201698303\n",
      "Iteration #210 loss: 0.21264445781707764\n",
      "Iteration #220 loss: 0.25376611948013306\n",
      "Iteration #230 loss: 0.08931796997785568\n",
      "Iteration #240 loss: 0.08179635554552078\n",
      "Iteration #250 loss: 0.46119987964630127\n",
      "Iteration #260 loss: 0.21689748764038086\n",
      "Iteration #270 loss: 0.3509524166584015\n",
      "Iteration #280 loss: 0.10907916724681854\n",
      "Iteration #290 loss: 0.11587359011173248\n",
      "Iteration #300 loss: 0.19138547778129578\n",
      "Iteration #310 loss: 0.13356716930866241\n",
      "Iteration #320 loss: 0.2402394413948059\n",
      "Iteration #330 loss: 0.09466883540153503\n",
      "Iteration #340 loss: 0.16738441586494446\n",
      "Iteration #350 loss: 0.2873133718967438\n",
      "Iteration #360 loss: 0.263396292924881\n",
      "Iteration #370 loss: 0.11252863705158234\n",
      "Iteration #380 loss: 0.26235517859458923\n",
      "Iteration #390 loss: 0.06346578896045685\n",
      "Iteration #400 loss: 0.18823038041591644\n",
      "Iteration #410 loss: 0.18748776614665985\n",
      "Iteration #420 loss: 0.2285270243883133\n",
      "Epoch #14 average loss: 0.24583646444484344\n",
      "Epoch #14 accuracy: 0.07458233890214797\n",
      "Iteration #10 loss: 0.4572792649269104\n",
      "Iteration #20 loss: 0.22364585101604462\n",
      "Iteration #30 loss: 0.31248337030410767\n",
      "Iteration #40 loss: 0.2623189389705658\n",
      "Iteration #50 loss: 0.18148890137672424\n",
      "Iteration #60 loss: 0.21308083832263947\n",
      "Iteration #70 loss: 0.15366223454475403\n",
      "Iteration #80 loss: 0.3440186083316803\n",
      "Iteration #90 loss: 0.4187692701816559\n",
      "Iteration #100 loss: 0.1558292806148529\n",
      "Iteration #110 loss: 0.17554566264152527\n",
      "Iteration #120 loss: 0.2120043784379959\n",
      "Iteration #130 loss: 0.18113110959529877\n",
      "Iteration #140 loss: 0.16389109194278717\n",
      "Iteration #150 loss: 0.137828066945076\n",
      "Iteration #160 loss: 0.17348311841487885\n",
      "Iteration #170 loss: 0.3013349175453186\n",
      "Iteration #180 loss: 0.19955264031887054\n",
      "Iteration #190 loss: 0.27605727314949036\n",
      "Iteration #200 loss: 0.10386662185192108\n",
      "Iteration #210 loss: 0.1649639755487442\n",
      "Iteration #220 loss: 0.31858351826667786\n",
      "Iteration #230 loss: 0.29697704315185547\n",
      "Iteration #240 loss: 0.31696081161499023\n",
      "Iteration #250 loss: 0.3016365170478821\n",
      "Iteration #260 loss: 0.14797565340995789\n",
      "Iteration #270 loss: 0.1693796068429947\n",
      "Iteration #280 loss: 0.17748959362506866\n",
      "Iteration #290 loss: 0.148525208234787\n",
      "Iteration #300 loss: 0.3482872247695923\n",
      "Iteration #310 loss: 0.23040644824504852\n",
      "Iteration #320 loss: 0.09644877910614014\n",
      "Iteration #330 loss: 0.1027306616306305\n",
      "Iteration #340 loss: 0.1850508451461792\n",
      "Iteration #350 loss: 0.14840157330036163\n",
      "Iteration #360 loss: 0.0653429925441742\n",
      "Iteration #370 loss: 0.17514315247535706\n",
      "Iteration #380 loss: 0.10139542073011398\n",
      "Iteration #390 loss: 0.29951438307762146\n",
      "Iteration #400 loss: 0.13149648904800415\n",
      "Iteration #410 loss: 0.3719448149204254\n",
      "Iteration #420 loss: 0.15144751965999603\n",
      "Epoch #15 average loss: 0.23178874149104478\n",
      "Epoch #15 accuracy: 0.1032057572783775\n",
      "Iteration #10 loss: 0.21879306435585022\n",
      "Iteration #20 loss: 0.13989989459514618\n",
      "Iteration #30 loss: 0.306796669960022\n",
      "Iteration #40 loss: 0.1453479677438736\n",
      "Iteration #50 loss: 0.20212523639202118\n",
      "Iteration #60 loss: 0.22155439853668213\n",
      "Iteration #70 loss: 0.23836322128772736\n",
      "Iteration #80 loss: 0.24484361708164215\n",
      "Iteration #90 loss: 0.21009327471256256\n",
      "Iteration #100 loss: 0.2318667471408844\n",
      "Iteration #110 loss: 0.1882496476173401\n",
      "Iteration #120 loss: 0.20474094152450562\n",
      "Iteration #130 loss: 0.27917200326919556\n",
      "Iteration #140 loss: 0.27133771777153015\n",
      "Iteration #150 loss: 0.2195255160331726\n",
      "Iteration #160 loss: 0.1761637032032013\n",
      "Iteration #170 loss: 0.16302086412906647\n",
      "Iteration #180 loss: 0.13553033769130707\n",
      "Iteration #190 loss: 0.15521550178527832\n",
      "Iteration #200 loss: 0.2120426893234253\n",
      "Iteration #210 loss: 0.4030432105064392\n",
      "Iteration #220 loss: 0.13614651560783386\n",
      "Iteration #230 loss: 0.18244928121566772\n",
      "Iteration #240 loss: 0.3121660649776459\n",
      "Iteration #250 loss: 0.16045433282852173\n",
      "Iteration #260 loss: 0.07026741653680801\n",
      "Iteration #270 loss: 0.26370343565940857\n",
      "Iteration #280 loss: 0.16534562408924103\n",
      "Iteration #290 loss: 0.36432379484176636\n",
      "Iteration #300 loss: 0.26397964358329773\n",
      "Iteration #310 loss: 0.12896133959293365\n",
      "Iteration #320 loss: 0.26673057675361633\n",
      "Iteration #330 loss: 0.22897224128246307\n",
      "Iteration #340 loss: 0.17085090279579163\n",
      "Iteration #350 loss: 0.18092626333236694\n",
      "Iteration #360 loss: 0.18040378391742706\n",
      "Iteration #370 loss: 0.1033511757850647\n",
      "Iteration #380 loss: 0.1757059246301651\n",
      "Iteration #390 loss: 0.20793598890304565\n",
      "Iteration #400 loss: 0.14821387827396393\n",
      "Iteration #410 loss: 0.19875574111938477\n",
      "Iteration #420 loss: 0.2631538510322571\n",
      "Epoch #16 average loss: 0.21941258656546792\n",
      "Epoch #16 accuracy: 0.11740216486261448\n",
      "Iteration #10 loss: 0.25917986035346985\n",
      "Iteration #20 loss: 0.10543425381183624\n",
      "Iteration #30 loss: 0.15912176668643951\n",
      "Iteration #40 loss: 0.12637142837047577\n",
      "Iteration #50 loss: 0.20934675633907318\n",
      "Iteration #60 loss: 0.07816310226917267\n",
      "Iteration #70 loss: 0.1283678561449051\n",
      "Iteration #80 loss: 0.39529773592948914\n",
      "Iteration #90 loss: 0.18625593185424805\n",
      "Iteration #100 loss: 0.132846862077713\n",
      "Iteration #110 loss: 0.22639316320419312\n",
      "Iteration #120 loss: 0.21119004487991333\n",
      "Iteration #130 loss: 0.3037199378013611\n",
      "Iteration #140 loss: 0.2460588812828064\n",
      "Iteration #150 loss: 0.25713789463043213\n",
      "Iteration #160 loss: 0.20441222190856934\n",
      "Iteration #170 loss: 0.13738001883029938\n",
      "Iteration #180 loss: 0.25593680143356323\n",
      "Iteration #190 loss: 0.15889647603034973\n",
      "Iteration #200 loss: 0.0961645171046257\n",
      "Iteration #210 loss: 0.11301711201667786\n",
      "Iteration #220 loss: 0.23250071704387665\n",
      "Iteration #230 loss: 0.2170606255531311\n",
      "Iteration #240 loss: 0.24765169620513916\n",
      "Iteration #250 loss: 0.3579443395137787\n",
      "Iteration #260 loss: 0.3232872486114502\n",
      "Iteration #270 loss: 0.12790973484516144\n",
      "Iteration #280 loss: 0.1678580641746521\n",
      "Iteration #290 loss: 0.22320479154586792\n",
      "Iteration #300 loss: 0.12272987514734268\n",
      "Iteration #310 loss: 0.32261061668395996\n",
      "Iteration #320 loss: 0.08598121255636215\n",
      "Iteration #330 loss: 0.15171635150909424\n",
      "Iteration #340 loss: 0.12917941808700562\n",
      "Iteration #350 loss: 0.19209939241409302\n",
      "Iteration #360 loss: 0.20858412981033325\n",
      "Iteration #370 loss: 0.15698979794979095\n",
      "Iteration #380 loss: 0.4662005305290222\n",
      "Iteration #390 loss: 0.12807494401931763\n",
      "Iteration #400 loss: 0.17667528986930847\n",
      "Iteration #410 loss: 0.1601121723651886\n",
      "Iteration #420 loss: 0.11445312201976776\n",
      "Epoch #17 average loss: 0.2093987707977765\n",
      "Epoch #17 accuracy: 0.10981668665410313\n",
      "Iteration #10 loss: 0.17925551533699036\n",
      "Iteration #20 loss: 0.23384921252727509\n",
      "Iteration #30 loss: 0.0935433879494667\n",
      "Iteration #40 loss: 0.29151761531829834\n",
      "Iteration #50 loss: 0.12418228387832642\n",
      "Iteration #60 loss: 0.18706023693084717\n",
      "Iteration #70 loss: 0.3549518585205078\n",
      "Iteration #80 loss: 0.09320595860481262\n",
      "Iteration #90 loss: 0.30834436416625977\n",
      "Iteration #100 loss: 0.18962764739990234\n",
      "Iteration #110 loss: 0.10277390480041504\n",
      "Iteration #120 loss: 0.35645902156829834\n",
      "Iteration #130 loss: 0.12771373987197876\n",
      "Iteration #140 loss: 0.23515939712524414\n",
      "Iteration #150 loss: 0.11289099603891373\n",
      "Iteration #160 loss: 0.10028735548257828\n",
      "Iteration #170 loss: 0.07443500310182571\n",
      "Iteration #180 loss: 0.20771968364715576\n",
      "Iteration #190 loss: 0.0831550657749176\n",
      "Iteration #200 loss: 0.1334468126296997\n",
      "Iteration #210 loss: 0.16589686274528503\n",
      "Iteration #220 loss: 0.2442859709262848\n",
      "Iteration #230 loss: 0.24141857028007507\n",
      "Iteration #240 loss: 0.09784270823001862\n",
      "Iteration #250 loss: 0.23378048837184906\n",
      "Iteration #260 loss: 0.2637644410133362\n",
      "Iteration #270 loss: 0.2112044095993042\n",
      "Iteration #280 loss: 0.22812066972255707\n",
      "Iteration #290 loss: 0.3339991569519043\n",
      "Iteration #300 loss: 0.2883843183517456\n",
      "Iteration #310 loss: 0.26631322503089905\n",
      "Iteration #320 loss: 0.2126636803150177\n",
      "Iteration #330 loss: 0.28994518518447876\n",
      "Iteration #340 loss: 0.1502527892589569\n",
      "Iteration #350 loss: 0.20108872652053833\n",
      "Iteration #360 loss: 0.10694514214992523\n",
      "Iteration #370 loss: 0.14216691255569458\n",
      "Iteration #380 loss: 0.09238949418067932\n",
      "Iteration #390 loss: 0.16058458387851715\n",
      "Iteration #400 loss: 0.2149232178926468\n",
      "Iteration #410 loss: 0.11686310172080994\n",
      "Iteration #420 loss: 0.24123671650886536\n",
      "Epoch #18 average loss: 0.2029770625016327\n",
      "Epoch #18 accuracy: 0.10239018087855298\n",
      "Iteration #10 loss: 0.23036767542362213\n",
      "Iteration #20 loss: 0.1892096847295761\n",
      "Iteration #30 loss: 0.27695009112358093\n",
      "Iteration #40 loss: 0.14640893042087555\n",
      "Iteration #50 loss: 0.13864199817180634\n",
      "Iteration #60 loss: 0.18523824214935303\n",
      "Iteration #70 loss: 0.4084659516811371\n",
      "Iteration #80 loss: 0.10627052932977676\n",
      "Iteration #90 loss: 0.12124235928058624\n",
      "Iteration #100 loss: 0.13122832775115967\n",
      "Iteration #110 loss: 0.31367185711860657\n",
      "Iteration #120 loss: 0.19381409883499146\n",
      "Iteration #130 loss: 0.18599162995815277\n",
      "Iteration #140 loss: 0.1396116018295288\n",
      "Iteration #150 loss: 0.2346840351819992\n",
      "Iteration #160 loss: 0.10752878338098526\n",
      "Iteration #170 loss: 0.14270997047424316\n",
      "Iteration #180 loss: 0.28839433193206787\n",
      "Iteration #190 loss: 0.11048036813735962\n",
      "Iteration #200 loss: 0.27285128831863403\n",
      "Iteration #210 loss: 0.15529021620750427\n",
      "Iteration #220 loss: 0.30838119983673096\n",
      "Iteration #230 loss: 0.13737651705741882\n",
      "Iteration #240 loss: 0.24706172943115234\n",
      "Iteration #250 loss: 0.16761675477027893\n",
      "Iteration #260 loss: 0.18562959134578705\n",
      "Iteration #270 loss: 0.16549216210842133\n",
      "Iteration #280 loss: 0.13103477656841278\n",
      "Iteration #290 loss: 0.16705001890659332\n",
      "Iteration #300 loss: 0.09716189652681351\n",
      "Iteration #310 loss: 0.1785944700241089\n",
      "Iteration #320 loss: 0.5512006282806396\n",
      "Iteration #330 loss: 0.09245212376117706\n",
      "Iteration #340 loss: 0.27449092268943787\n",
      "Iteration #350 loss: 0.19930227100849152\n",
      "Iteration #360 loss: 0.22776779532432556\n",
      "Iteration #370 loss: 0.1394147425889969\n",
      "Iteration #380 loss: 0.13294564187526703\n",
      "Iteration #390 loss: 0.231338769197464\n",
      "Iteration #400 loss: 0.4503506124019623\n",
      "Iteration #410 loss: 0.20752497017383575\n",
      "Iteration #420 loss: 0.07474686205387115\n",
      "Epoch #19 average loss: 0.19361178160173875\n",
      "Epoch #19 accuracy: 0.11241641062714622\n",
      "Iteration #10 loss: 0.14103896915912628\n",
      "Iteration #20 loss: 0.33507010340690613\n",
      "Iteration #30 loss: 0.266103595495224\n",
      "Iteration #40 loss: 0.10957494378089905\n",
      "Iteration #50 loss: 0.17584049701690674\n",
      "Iteration #60 loss: 0.21179524064064026\n",
      "Iteration #70 loss: 0.22203616797924042\n",
      "Iteration #80 loss: 0.22267261147499084\n",
      "Iteration #90 loss: 0.206808939576149\n",
      "Iteration #100 loss: 0.058955371379852295\n",
      "Iteration #110 loss: 0.2662488520145416\n",
      "Iteration #120 loss: 0.21317261457443237\n",
      "Iteration #130 loss: 0.19234512746334076\n",
      "Iteration #140 loss: 0.19122152030467987\n",
      "Iteration #150 loss: 0.11003217101097107\n",
      "Iteration #160 loss: 0.21043352782726288\n",
      "Iteration #170 loss: 0.07662822306156158\n",
      "Iteration #180 loss: 0.13767573237419128\n",
      "Iteration #190 loss: 0.12269261479377747\n",
      "Iteration #200 loss: 0.13716205954551697\n",
      "Iteration #210 loss: 0.29957279562950134\n",
      "Iteration #220 loss: 0.11370457708835602\n",
      "Iteration #230 loss: 0.23988735675811768\n",
      "Iteration #240 loss: 0.2465354949235916\n",
      "Iteration #250 loss: 0.14264535903930664\n",
      "Iteration #260 loss: 0.2946494519710541\n",
      "Iteration #270 loss: 0.10844589024782181\n",
      "Iteration #280 loss: 0.15368643403053284\n",
      "Iteration #290 loss: 0.3219142556190491\n",
      "Iteration #300 loss: 0.1976228803396225\n",
      "Iteration #310 loss: 0.24799862504005432\n",
      "Iteration #320 loss: 0.07202816009521484\n",
      "Iteration #330 loss: 0.15601478517055511\n",
      "Iteration #340 loss: 0.22753947973251343\n",
      "Iteration #350 loss: 0.1506936252117157\n",
      "Iteration #360 loss: 0.24171940982341766\n",
      "Iteration #370 loss: 0.23205533623695374\n",
      "Iteration #380 loss: 0.10258699953556061\n",
      "Iteration #390 loss: 0.2827500104904175\n",
      "Iteration #400 loss: 0.16904370486736298\n",
      "Iteration #410 loss: 0.18611688911914825\n",
      "Iteration #420 loss: 0.16086076200008392\n",
      "Epoch #20 average loss: 0.18659362231432117\n",
      "Epoch #20 accuracy: 0.11158056462454388\n",
      "Iteration #10 loss: 0.13816389441490173\n",
      "Iteration #20 loss: 0.32441458106040955\n",
      "Iteration #30 loss: 0.10764209181070328\n",
      "Iteration #40 loss: 0.14779405295848846\n",
      "Iteration #50 loss: 0.22093111276626587\n",
      "Iteration #60 loss: 0.2855634391307831\n",
      "Iteration #70 loss: 0.25345876812934875\n",
      "Iteration #80 loss: 0.20352980494499207\n",
      "Iteration #90 loss: 0.2050236016511917\n",
      "Iteration #100 loss: 0.1273658126592636\n",
      "Iteration #110 loss: 0.22131359577178955\n",
      "Iteration #120 loss: 0.3369208872318268\n",
      "Iteration #130 loss: 0.31351256370544434\n",
      "Iteration #140 loss: 0.16358086466789246\n",
      "Iteration #150 loss: 0.09701527655124664\n",
      "Iteration #160 loss: 0.4260886013507843\n",
      "Iteration #170 loss: 0.2545033097267151\n",
      "Iteration #180 loss: 0.15462937951087952\n",
      "Iteration #190 loss: 0.18646986782550812\n",
      "Iteration #200 loss: 0.10746108740568161\n",
      "Iteration #210 loss: 0.08062681555747986\n",
      "Iteration #220 loss: 0.07801744341850281\n",
      "Iteration #230 loss: 0.06197946518659592\n",
      "Iteration #240 loss: 0.11921101808547974\n",
      "Iteration #250 loss: 0.0886884480714798\n",
      "Iteration #260 loss: 0.1650443971157074\n",
      "Iteration #270 loss: 0.2577609419822693\n",
      "Iteration #280 loss: 0.06937916576862335\n",
      "Iteration #290 loss: 0.22569888830184937\n",
      "Iteration #300 loss: 0.2218436896800995\n",
      "Iteration #310 loss: 0.11916157603263855\n",
      "Iteration #320 loss: 0.153508722782135\n",
      "Iteration #330 loss: 0.2791764438152313\n",
      "Iteration #340 loss: 0.22632935643196106\n",
      "Iteration #350 loss: 0.11819753795862198\n",
      "Iteration #360 loss: 0.10379048436880112\n",
      "Iteration #370 loss: 0.18483974039554596\n",
      "Iteration #380 loss: 0.15470968186855316\n",
      "Iteration #390 loss: 0.2160434126853943\n",
      "Iteration #400 loss: 0.15093360841274261\n",
      "Iteration #410 loss: 0.11861475557088852\n",
      "Iteration #420 loss: 0.16930457949638367\n",
      "Epoch #21 average loss: 0.18002956745495705\n",
      "Epoch #21 accuracy: 0.1242263483642794\n",
      "Iteration #10 loss: 0.24591411650180817\n",
      "Iteration #20 loss: 0.2605685293674469\n",
      "Iteration #30 loss: 0.18955771625041962\n",
      "Iteration #40 loss: 0.22276422381401062\n",
      "Iteration #50 loss: 0.14731617271900177\n",
      "Iteration #60 loss: 0.28044286370277405\n",
      "Iteration #70 loss: 0.24585562944412231\n",
      "Iteration #80 loss: 0.13503143191337585\n",
      "Iteration #90 loss: 0.3818870186805725\n",
      "Iteration #100 loss: 0.19898621737957\n",
      "Iteration #110 loss: 0.15385305881500244\n",
      "Iteration #120 loss: 0.25569403171539307\n",
      "Iteration #130 loss: 0.2455647885799408\n",
      "Iteration #140 loss: 0.09632454812526703\n",
      "Iteration #150 loss: 0.2160971760749817\n",
      "Iteration #160 loss: 0.1366575062274933\n",
      "Iteration #170 loss: 0.19860002398490906\n",
      "Iteration #180 loss: 0.13677047193050385\n",
      "Iteration #190 loss: 0.17323940992355347\n",
      "Iteration #200 loss: 0.3716300129890442\n",
      "Iteration #210 loss: 0.11324476450681686\n",
      "Iteration #220 loss: 0.27439630031585693\n",
      "Iteration #230 loss: 0.22498811781406403\n",
      "Iteration #240 loss: 0.14900502562522888\n",
      "Iteration #250 loss: 0.19671016931533813\n",
      "Iteration #260 loss: 0.141617089509964\n",
      "Iteration #270 loss: 0.18513108789920807\n",
      "Iteration #280 loss: 0.09976739436388016\n",
      "Iteration #290 loss: 0.07159755378961563\n",
      "Iteration #300 loss: 0.20710664987564087\n",
      "Iteration #310 loss: 0.19642457365989685\n",
      "Iteration #320 loss: 0.11306441575288773\n",
      "Iteration #330 loss: 0.08530586212873459\n",
      "Iteration #340 loss: 0.14392948150634766\n",
      "Iteration #350 loss: 0.1119781881570816\n",
      "Iteration #360 loss: 0.2410135716199875\n",
      "Iteration #370 loss: 0.17518363893032074\n",
      "Iteration #380 loss: 0.22174504399299622\n",
      "Iteration #390 loss: 0.07218770682811737\n",
      "Iteration #400 loss: 0.1950104981660843\n",
      "Iteration #410 loss: 0.135944664478302\n",
      "Iteration #420 loss: 0.21045885980129242\n",
      "Epoch #22 average loss: 0.1695744217309278\n",
      "Epoch #22 accuracy: 0.11923307369682444\n",
      "Iteration #10 loss: 0.17609182000160217\n",
      "Iteration #20 loss: 0.08837980031967163\n",
      "Iteration #30 loss: 0.045708656311035156\n",
      "Iteration #40 loss: 0.2507020831108093\n",
      "Iteration #50 loss: 0.19819912314414978\n",
      "Iteration #60 loss: 0.18459203839302063\n",
      "Iteration #70 loss: 0.22182874381542206\n",
      "Iteration #80 loss: 0.2375904768705368\n",
      "Iteration #90 loss: 0.06769216805696487\n",
      "Iteration #100 loss: 0.18833361566066742\n",
      "Iteration #110 loss: 0.15229399502277374\n",
      "Iteration #120 loss: 0.0819006860256195\n",
      "Iteration #130 loss: 0.23237250745296478\n",
      "Iteration #140 loss: 0.1919751614332199\n",
      "Iteration #150 loss: 0.09826845675706863\n",
      "Iteration #160 loss: 0.24715708196163177\n",
      "Iteration #170 loss: 0.09524624794721603\n",
      "Iteration #180 loss: 0.19322437047958374\n",
      "Iteration #190 loss: 0.20393846929073334\n",
      "Iteration #200 loss: 0.1792011857032776\n",
      "Iteration #210 loss: 0.16618557274341583\n",
      "Iteration #220 loss: 0.23639658093452454\n",
      "Iteration #230 loss: 0.08769362419843674\n",
      "Iteration #240 loss: 0.18192799389362335\n",
      "Iteration #250 loss: 0.28664642572402954\n",
      "Iteration #260 loss: 0.14395228028297424\n",
      "Iteration #270 loss: 0.11621885001659393\n",
      "Iteration #280 loss: 0.19271576404571533\n",
      "Iteration #290 loss: 0.2422429323196411\n",
      "Iteration #300 loss: 0.2700367867946625\n",
      "Iteration #310 loss: 0.2706066966056824\n",
      "Iteration #320 loss: 0.18518321216106415\n",
      "Iteration #330 loss: 0.12119290232658386\n",
      "Iteration #340 loss: 0.1312260627746582\n",
      "Iteration #350 loss: 0.11702556908130646\n",
      "Iteration #360 loss: 0.14434988796710968\n",
      "Iteration #370 loss: 0.16842994093894958\n",
      "Iteration #380 loss: 0.1593424677848816\n",
      "Iteration #390 loss: 0.1981043815612793\n",
      "Iteration #400 loss: 0.06482004374265671\n",
      "Iteration #410 loss: 0.17729701101779938\n",
      "Iteration #420 loss: 0.12397811561822891\n",
      "Epoch #23 average loss: 0.1672164331546447\n",
      "Epoch #23 accuracy: 0.11650701715304077\n",
      "Iteration #10 loss: 0.12879471480846405\n",
      "Iteration #20 loss: 0.37530452013015747\n",
      "Iteration #30 loss: 0.18313463032245636\n",
      "Iteration #40 loss: 0.34518107771873474\n",
      "Iteration #50 loss: 0.19564484059810638\n",
      "Iteration #60 loss: 0.10236737877130508\n",
      "Iteration #70 loss: 0.24752329289913177\n",
      "Iteration #80 loss: 0.15748231112957\n",
      "Iteration #90 loss: 0.2393133044242859\n",
      "Iteration #100 loss: 0.20639801025390625\n",
      "Iteration #110 loss: 0.19502049684524536\n",
      "Iteration #120 loss: 0.069534070789814\n",
      "Iteration #130 loss: 0.19448387622833252\n",
      "Iteration #140 loss: 0.16532248258590698\n",
      "Iteration #150 loss: 0.1630931943655014\n",
      "Iteration #160 loss: 0.10514836758375168\n",
      "Iteration #170 loss: 0.06265070289373398\n",
      "Iteration #180 loss: 0.042948223650455475\n",
      "Iteration #190 loss: 0.07232023775577545\n",
      "Iteration #200 loss: 0.06211356073617935\n",
      "Iteration #210 loss: 0.09781673550605774\n",
      "Iteration #220 loss: 0.16533410549163818\n",
      "Iteration #230 loss: 0.07438822835683823\n",
      "Iteration #240 loss: 0.08875490725040436\n",
      "Iteration #250 loss: 0.1491745561361313\n",
      "Iteration #260 loss: 0.21866898238658905\n",
      "Iteration #270 loss: 0.18917931616306305\n",
      "Iteration #280 loss: 0.1370747834444046\n",
      "Iteration #290 loss: 0.05019284784793854\n",
      "Iteration #300 loss: 0.24321860074996948\n",
      "Iteration #310 loss: 0.06459051370620728\n",
      "Iteration #320 loss: 0.19993318617343903\n",
      "Iteration #330 loss: 0.10966042429208755\n",
      "Iteration #340 loss: 0.20237232744693756\n",
      "Iteration #350 loss: 0.08337248861789703\n",
      "Iteration #360 loss: 0.14988252520561218\n",
      "Iteration #370 loss: 0.1598110944032669\n",
      "Iteration #380 loss: 0.24118393659591675\n",
      "Iteration #390 loss: 0.08198042958974838\n",
      "Iteration #400 loss: 0.12361377477645874\n",
      "Iteration #410 loss: 0.2787632942199707\n",
      "Iteration #420 loss: 0.21937468647956848\n",
      "Epoch #24 average loss: 0.16031780260263317\n",
      "Epoch #24 accuracy: 0.11005089058524173\n",
      "Iteration #10 loss: 0.1172904372215271\n",
      "Iteration #20 loss: 0.07455515116453171\n",
      "Iteration #30 loss: 0.24608221650123596\n",
      "Iteration #40 loss: 0.20071248710155487\n",
      "Iteration #50 loss: 0.1948785036802292\n",
      "Iteration #60 loss: 0.08755377680063248\n",
      "Iteration #70 loss: 0.26937344670295715\n",
      "Iteration #80 loss: 0.09897955507040024\n",
      "Iteration #90 loss: 0.3619001805782318\n",
      "Iteration #100 loss: 0.14466817677021027\n",
      "Iteration #110 loss: 0.22993354499340057\n",
      "Iteration #120 loss: 0.22123076021671295\n",
      "Iteration #130 loss: 0.14059561491012573\n",
      "Iteration #140 loss: 0.12608012557029724\n",
      "Iteration #150 loss: 0.09716399759054184\n",
      "Iteration #160 loss: 0.25722646713256836\n",
      "Iteration #170 loss: 0.1229160875082016\n",
      "Iteration #180 loss: 0.24857835471630096\n",
      "Iteration #190 loss: 0.26483649015426636\n",
      "Iteration #200 loss: 0.08510014414787292\n",
      "Iteration #210 loss: 0.2540380358695984\n",
      "Iteration #220 loss: 0.09221755713224411\n",
      "Iteration #230 loss: 0.1061263233423233\n",
      "Iteration #240 loss: 0.09525143355131149\n",
      "Iteration #250 loss: 0.3405728340148926\n",
      "Iteration #260 loss: 0.2779083251953125\n",
      "Iteration #270 loss: 0.24115218222141266\n",
      "Iteration #280 loss: 0.08680155128240585\n",
      "Iteration #290 loss: 0.22140637040138245\n",
      "Iteration #300 loss: 0.16865378618240356\n",
      "Iteration #310 loss: 0.09711896628141403\n",
      "Iteration #320 loss: 0.2752661108970642\n",
      "Iteration #330 loss: 0.15823978185653687\n",
      "Iteration #340 loss: 0.13583630323410034\n",
      "Iteration #350 loss: 0.11373845487833023\n",
      "Iteration #360 loss: 0.0689152404665947\n",
      "Iteration #370 loss: 0.09273851662874222\n",
      "Iteration #380 loss: 0.07873187959194183\n",
      "Iteration #390 loss: 0.20201173424720764\n",
      "Iteration #400 loss: 0.2551364600658417\n",
      "Iteration #410 loss: 0.16533833742141724\n",
      "Iteration #420 loss: 0.20871320366859436\n",
      "Epoch #25 average loss: 0.1521118743579631\n",
      "Epoch #25 accuracy: 0.13191832348199892\n",
      "Iteration #10 loss: 0.10053832828998566\n",
      "Iteration #20 loss: 0.15970920026302338\n",
      "Iteration #30 loss: 0.18764153122901917\n",
      "Iteration #40 loss: 0.13300997018814087\n",
      "Iteration #50 loss: 0.14832738041877747\n",
      "Iteration #60 loss: 0.1221727579832077\n",
      "Iteration #70 loss: 0.12114529311656952\n",
      "Iteration #80 loss: 0.060645416378974915\n",
      "Iteration #90 loss: 0.06619173288345337\n",
      "Iteration #100 loss: 0.11645149439573288\n",
      "Iteration #110 loss: 0.12801803648471832\n",
      "Iteration #120 loss: 0.11092565953731537\n",
      "Iteration #130 loss: 0.26245325803756714\n",
      "Iteration #140 loss: 0.245869979262352\n",
      "Iteration #150 loss: 0.0678468719124794\n",
      "Iteration #160 loss: 0.23220959305763245\n",
      "Iteration #170 loss: 0.12845063209533691\n",
      "Iteration #180 loss: 0.21512176096439362\n",
      "Iteration #190 loss: 0.21406280994415283\n",
      "Iteration #200 loss: 0.14853765070438385\n",
      "Iteration #210 loss: 0.07299207150936127\n",
      "Iteration #220 loss: 0.18235810101032257\n",
      "Iteration #230 loss: 0.08756788820028305\n",
      "Iteration #240 loss: 0.0961465910077095\n",
      "Iteration #250 loss: 0.09560641646385193\n",
      "Iteration #260 loss: 0.1438017636537552\n",
      "Iteration #270 loss: 0.0949513167142868\n",
      "Iteration #280 loss: 0.2028365582227707\n",
      "Iteration #290 loss: 0.10132573544979095\n",
      "Iteration #300 loss: 0.10845088958740234\n",
      "Iteration #310 loss: 0.0847553089261055\n",
      "Iteration #320 loss: 0.10169106721878052\n",
      "Iteration #330 loss: 0.1654922068119049\n",
      "Iteration #340 loss: 0.15262353420257568\n",
      "Iteration #350 loss: 0.03865830600261688\n",
      "Iteration #360 loss: 0.2053813934326172\n",
      "Iteration #370 loss: 0.07595935463905334\n",
      "Iteration #380 loss: 0.22291633486747742\n",
      "Iteration #390 loss: 0.22818174958229065\n",
      "Iteration #400 loss: 0.12549160420894623\n",
      "Iteration #410 loss: 0.05126240849494934\n",
      "Iteration #420 loss: 0.16960486769676208\n",
      "Epoch #26 average loss: 0.14948116580904805\n",
      "Epoch #26 accuracy: 0.12683186783906208\n",
      "Iteration #10 loss: 0.19118158519268036\n",
      "Iteration #20 loss: 0.07644837349653244\n",
      "Iteration #30 loss: 0.1393427550792694\n",
      "Iteration #40 loss: 0.1658298671245575\n",
      "Iteration #50 loss: 0.22340741753578186\n",
      "Iteration #60 loss: 0.15923337638378143\n",
      "Iteration #70 loss: 0.14849650859832764\n",
      "Iteration #80 loss: 0.3618987500667572\n",
      "Iteration #90 loss: 0.1733814775943756\n",
      "Iteration #100 loss: 0.13207097351551056\n",
      "Iteration #110 loss: 0.1383994221687317\n",
      "Iteration #120 loss: 0.09511981159448624\n",
      "Iteration #130 loss: 0.08462608605623245\n",
      "Iteration #140 loss: 0.1744309663772583\n",
      "Iteration #150 loss: 0.11825837939977646\n",
      "Iteration #160 loss: 0.08012761175632477\n",
      "Iteration #170 loss: 0.2438208907842636\n",
      "Iteration #180 loss: 0.10648469626903534\n",
      "Iteration #190 loss: 0.20319156348705292\n",
      "Iteration #200 loss: 0.1798672378063202\n",
      "Iteration #210 loss: 0.22584104537963867\n",
      "Iteration #220 loss: 0.18443652987480164\n",
      "Iteration #230 loss: 0.05472816899418831\n",
      "Iteration #240 loss: 0.4167907238006592\n",
      "Iteration #250 loss: 0.18557053804397583\n",
      "Iteration #260 loss: 0.08468040823936462\n",
      "Iteration #270 loss: 0.18444164097309113\n",
      "Iteration #280 loss: 0.05969100818037987\n",
      "Iteration #290 loss: 0.26615437865257263\n",
      "Iteration #300 loss: 0.12281373143196106\n",
      "Iteration #310 loss: 0.09641167521476746\n",
      "Iteration #320 loss: 0.12897177040576935\n",
      "Iteration #330 loss: 0.13197483122348785\n",
      "Iteration #340 loss: 0.0714651569724083\n",
      "Iteration #350 loss: 0.16538205742835999\n",
      "Iteration #360 loss: 0.1370941698551178\n",
      "Iteration #370 loss: 0.10437436401844025\n",
      "Iteration #380 loss: 0.045507095754146576\n",
      "Iteration #390 loss: 0.07352635264396667\n",
      "Iteration #400 loss: 0.06813468784093857\n",
      "Iteration #410 loss: 0.08981622010469437\n",
      "Iteration #420 loss: 0.16675081849098206\n",
      "Epoch #27 average loss: 0.14745693654706812\n",
      "Epoch #27 accuracy: 0.13397012000979672\n",
      "Iteration #10 loss: 0.2928585112094879\n",
      "Iteration #20 loss: 0.12400050461292267\n",
      "Iteration #30 loss: 0.31257903575897217\n",
      "Iteration #40 loss: 0.254951149225235\n",
      "Iteration #50 loss: 0.20116674900054932\n",
      "Iteration #60 loss: 0.16103528439998627\n",
      "Iteration #70 loss: 0.13130466639995575\n",
      "Iteration #80 loss: 0.20723560452461243\n",
      "Iteration #90 loss: 0.11087289452552795\n",
      "Iteration #100 loss: 0.18704500794410706\n",
      "Iteration #110 loss: 0.08848946541547775\n",
      "Iteration #120 loss: 0.11069837212562561\n",
      "Iteration #130 loss: 0.10392136126756668\n",
      "Iteration #140 loss: 0.12301623076200485\n",
      "Iteration #150 loss: 0.12478132545948029\n",
      "Iteration #160 loss: 0.15567444264888763\n",
      "Iteration #170 loss: 0.16917426884174347\n",
      "Iteration #180 loss: 0.1801781803369522\n",
      "Iteration #190 loss: 0.04469485580921173\n",
      "Iteration #200 loss: 0.05504477024078369\n",
      "Iteration #210 loss: 0.13956451416015625\n",
      "Iteration #220 loss: 0.2490134984254837\n",
      "Iteration #230 loss: 0.06672461330890656\n",
      "Iteration #240 loss: 0.08479790389537811\n",
      "Iteration #250 loss: 0.15632233023643494\n",
      "Iteration #260 loss: 0.09999088943004608\n",
      "Iteration #270 loss: 0.353109210729599\n",
      "Iteration #280 loss: 0.25802114605903625\n",
      "Iteration #290 loss: 0.23049496114253998\n",
      "Iteration #300 loss: 0.1663777232170105\n",
      "Iteration #310 loss: 0.1529977023601532\n",
      "Iteration #320 loss: 0.1634662002325058\n",
      "Iteration #330 loss: 0.18462936580181122\n",
      "Iteration #340 loss: 0.06531818211078644\n",
      "Iteration #350 loss: 0.09632378816604614\n",
      "Iteration #360 loss: 0.1330663561820984\n",
      "Iteration #370 loss: 0.13892976939678192\n",
      "Iteration #380 loss: 0.047723133116960526\n",
      "Iteration #390 loss: 0.13493217527866364\n",
      "Iteration #400 loss: 0.14000754058361053\n",
      "Iteration #410 loss: 0.14223560690879822\n",
      "Iteration #420 loss: 0.06911540776491165\n",
      "Epoch #28 average loss: 0.1448262287060467\n",
      "Epoch #28 accuracy: 0.14331108992691452\n",
      "Iteration #10 loss: 0.060817573219537735\n",
      "Iteration #20 loss: 0.0785955935716629\n",
      "Iteration #30 loss: 0.11462234705686569\n",
      "Iteration #40 loss: 0.2037065178155899\n",
      "Iteration #50 loss: 0.08988896012306213\n",
      "Iteration #60 loss: 0.19121170043945312\n",
      "Iteration #70 loss: 0.14009657502174377\n",
      "Iteration #80 loss: 0.15682479739189148\n",
      "Iteration #90 loss: 0.13808272778987885\n",
      "Iteration #100 loss: 0.07190604507923126\n",
      "Iteration #110 loss: 0.1668621301651001\n",
      "Iteration #120 loss: 0.09575855731964111\n",
      "Iteration #130 loss: 0.1481998711824417\n",
      "Iteration #140 loss: 0.20775488018989563\n",
      "Iteration #150 loss: 0.05784377455711365\n",
      "Iteration #160 loss: 0.16366787254810333\n",
      "Iteration #170 loss: 0.05251379311084747\n",
      "Iteration #180 loss: 0.1126457080245018\n",
      "Iteration #190 loss: 0.1135067343711853\n",
      "Iteration #200 loss: 0.12652528285980225\n",
      "Iteration #210 loss: 0.10672128200531006\n",
      "Iteration #220 loss: 0.14789219200611115\n",
      "Iteration #230 loss: 0.03590555861592293\n",
      "Iteration #240 loss: 0.14075128734111786\n",
      "Iteration #250 loss: 0.048323601484298706\n",
      "Iteration #260 loss: 0.1755433976650238\n",
      "Iteration #270 loss: 0.12424737960100174\n",
      "Iteration #280 loss: 0.06717093288898468\n",
      "Iteration #290 loss: 0.08164245635271072\n",
      "Iteration #300 loss: 0.11453915387392044\n",
      "Iteration #310 loss: 0.14817185699939728\n",
      "Iteration #320 loss: 0.0876559391617775\n",
      "Iteration #330 loss: 0.08767888695001602\n",
      "Iteration #340 loss: 0.08719345927238464\n",
      "Iteration #350 loss: 0.12667889893054962\n",
      "Iteration #360 loss: 0.23926885426044464\n",
      "Iteration #370 loss: 0.13059653341770172\n",
      "Iteration #380 loss: 0.10315923392772675\n",
      "Iteration #390 loss: 0.1218254491686821\n",
      "Iteration #400 loss: 0.2294243574142456\n",
      "Iteration #410 loss: 0.06898069381713867\n",
      "Iteration #420 loss: 0.1730176955461502\n",
      "Epoch #29 average loss: 0.13745089498240137\n",
      "Epoch #29 accuracy: 0.16428832784485914\n",
      "Iteration #10 loss: 0.21167951822280884\n",
      "Iteration #20 loss: 0.1482740342617035\n",
      "Iteration #30 loss: 0.04736611619591713\n",
      "Iteration #40 loss: 0.17234566807746887\n",
      "Iteration #50 loss: 0.18756549060344696\n",
      "Iteration #60 loss: 0.0690826028585434\n",
      "Iteration #70 loss: 0.08644219487905502\n",
      "Iteration #80 loss: 0.2542368471622467\n",
      "Iteration #90 loss: 0.18442559242248535\n",
      "Iteration #100 loss: 0.09997943788766861\n",
      "Iteration #110 loss: 0.17396564781665802\n",
      "Iteration #120 loss: 0.19788508117198944\n",
      "Iteration #130 loss: 0.06569679081439972\n",
      "Iteration #140 loss: 0.08372950553894043\n",
      "Iteration #150 loss: 0.3078800439834595\n",
      "Iteration #160 loss: 0.12291349470615387\n",
      "Iteration #170 loss: 0.1740269958972931\n",
      "Iteration #180 loss: 0.08692152053117752\n",
      "Iteration #190 loss: 0.11690863966941833\n",
      "Iteration #200 loss: 0.1186598613858223\n",
      "Iteration #210 loss: 0.1724686622619629\n",
      "Iteration #220 loss: 0.09631388634443283\n",
      "Iteration #230 loss: 0.18895354866981506\n",
      "Iteration #240 loss: 0.09388066828250885\n",
      "Iteration #250 loss: 0.08081503212451935\n",
      "Iteration #260 loss: 0.1504632532596588\n",
      "Iteration #270 loss: 0.10262561589479446\n",
      "Iteration #280 loss: 0.23423418402671814\n",
      "Iteration #290 loss: 0.12852632999420166\n",
      "Iteration #300 loss: 0.09660463780164719\n",
      "Iteration #310 loss: 0.1690366566181183\n",
      "Iteration #320 loss: 0.12081723660230637\n",
      "Iteration #330 loss: 0.11815031617879868\n",
      "Iteration #340 loss: 0.2160462737083435\n",
      "Iteration #350 loss: 0.060134660452604294\n",
      "Iteration #360 loss: 0.02959294617176056\n",
      "Iteration #370 loss: 0.08347898721694946\n",
      "Iteration #380 loss: 0.06715307384729385\n",
      "Iteration #390 loss: 0.225991353392601\n",
      "Iteration #400 loss: 0.2655395567417145\n",
      "Iteration #410 loss: 0.11053130775690079\n",
      "Iteration #420 loss: 0.09776260703802109\n",
      "Epoch #30 average loss: 0.13336901620667507\n",
      "Epoch #30 accuracy: 0.16523605150214593\n",
      "Iteration #10 loss: 0.148330420255661\n",
      "Iteration #20 loss: 0.09201891720294952\n",
      "Iteration #30 loss: 0.13866807520389557\n",
      "Iteration #40 loss: 0.12692777812480927\n",
      "Iteration #50 loss: 0.07323454320430756\n",
      "Iteration #60 loss: 0.1793346107006073\n",
      "Iteration #70 loss: 0.170749694108963\n",
      "Iteration #80 loss: 0.08001118153333664\n",
      "Iteration #90 loss: 0.08285794407129288\n",
      "Iteration #100 loss: 0.08824235945940018\n",
      "Iteration #110 loss: 0.14943116903305054\n",
      "Iteration #120 loss: 0.1714952141046524\n",
      "Iteration #130 loss: 0.053373828530311584\n",
      "Iteration #140 loss: 0.1912434846162796\n",
      "Iteration #150 loss: 0.2403194010257721\n",
      "Iteration #160 loss: 0.1833118200302124\n",
      "Iteration #170 loss: 0.0725470706820488\n",
      "Iteration #180 loss: 0.05129930377006531\n",
      "Iteration #190 loss: 0.14319857954978943\n",
      "Iteration #200 loss: 0.04593006148934364\n",
      "Iteration #210 loss: 0.09656210243701935\n",
      "Iteration #220 loss: 0.112312912940979\n",
      "Iteration #230 loss: 0.14123062789440155\n",
      "Iteration #240 loss: 0.05500247702002525\n",
      "Iteration #250 loss: 0.0665082037448883\n",
      "Iteration #260 loss: 0.077967569231987\n",
      "Iteration #270 loss: 0.09304073452949524\n",
      "Iteration #280 loss: 0.10048434138298035\n",
      "Iteration #290 loss: 0.23854213953018188\n",
      "Iteration #300 loss: 0.12688054144382477\n",
      "Iteration #310 loss: 0.15968865156173706\n",
      "Iteration #320 loss: 0.07453306764364243\n",
      "Iteration #330 loss: 0.21254871785640717\n",
      "Iteration #340 loss: 0.07601676136255264\n",
      "Iteration #350 loss: 0.08917450904846191\n",
      "Iteration #360 loss: 0.16545261442661285\n",
      "Iteration #370 loss: 0.1867063045501709\n",
      "Iteration #380 loss: 0.036036670207977295\n",
      "Iteration #390 loss: 0.09095004200935364\n",
      "Iteration #400 loss: 0.062123723328113556\n",
      "Iteration #410 loss: 0.10125149041414261\n",
      "Iteration #420 loss: 0.14557844400405884\n",
      "Epoch #31 average loss: 0.12760655493022002\n",
      "Epoch #31 accuracy: 0.12751677852348994\n",
      "Iteration #10 loss: 0.1079336479306221\n",
      "Iteration #20 loss: 0.06436119973659515\n",
      "Iteration #30 loss: 0.0696745291352272\n",
      "Iteration #40 loss: 0.18344229459762573\n",
      "Iteration #50 loss: 0.1016523689031601\n",
      "Iteration #60 loss: 0.09618469327688217\n",
      "Iteration #70 loss: 0.06699804961681366\n",
      "Iteration #80 loss: 0.04149337112903595\n",
      "Iteration #90 loss: 0.08520393818616867\n",
      "Iteration #100 loss: 0.14980611205101013\n",
      "Iteration #110 loss: 0.06360912322998047\n",
      "Iteration #120 loss: 0.12300276756286621\n",
      "Iteration #130 loss: 0.07685615122318268\n",
      "Iteration #140 loss: 0.11518605053424835\n",
      "Iteration #150 loss: 0.07782606780529022\n",
      "Iteration #160 loss: 0.14700448513031006\n",
      "Iteration #170 loss: 0.027189690619707108\n",
      "Iteration #180 loss: 0.14355652034282684\n",
      "Iteration #190 loss: 0.15481029450893402\n",
      "Iteration #200 loss: 0.22835741937160492\n",
      "Iteration #210 loss: 0.13989171385765076\n",
      "Iteration #220 loss: 0.13556940853595734\n",
      "Iteration #230 loss: 0.08501818776130676\n",
      "Iteration #240 loss: 0.05451850965619087\n",
      "Iteration #250 loss: 0.14018487930297852\n",
      "Iteration #260 loss: 0.10204493999481201\n",
      "Iteration #270 loss: 0.14999493956565857\n",
      "Iteration #280 loss: 0.02925051376223564\n",
      "Iteration #290 loss: 0.06247276812791824\n",
      "Iteration #300 loss: 0.10323914140462875\n",
      "Iteration #310 loss: 0.10040181130170822\n",
      "Iteration #320 loss: 0.11496970802545547\n",
      "Iteration #330 loss: 0.12591956555843353\n",
      "Iteration #340 loss: 0.13018184900283813\n",
      "Iteration #350 loss: 0.07127367705106735\n",
      "Iteration #360 loss: 0.1281696856021881\n",
      "Iteration #370 loss: 0.13619430363178253\n",
      "Iteration #380 loss: 0.09644459187984467\n",
      "Iteration #390 loss: 0.10213652998209\n",
      "Iteration #400 loss: 0.09182840585708618\n",
      "Iteration #410 loss: 0.1262539178133011\n",
      "Iteration #420 loss: 0.1236727312207222\n",
      "Epoch #32 average loss: 0.1240826669876904\n",
      "Epoch #32 accuracy: 0.14642082429501085\n",
      "Iteration #10 loss: 0.17296040058135986\n",
      "Iteration #20 loss: 0.054591137915849686\n",
      "Iteration #30 loss: 0.2361597716808319\n",
      "Iteration #40 loss: 0.1227966845035553\n",
      "Iteration #50 loss: 0.09708301723003387\n",
      "Iteration #60 loss: 0.1351650506258011\n",
      "Iteration #70 loss: 0.21154744923114777\n",
      "Iteration #80 loss: 0.10622373223304749\n",
      "Iteration #90 loss: 0.1677050143480301\n",
      "Iteration #100 loss: 0.0906701534986496\n",
      "Iteration #110 loss: 0.059588875621557236\n",
      "Iteration #120 loss: 0.12288564443588257\n",
      "Iteration #130 loss: 0.15474867820739746\n",
      "Iteration #140 loss: 0.04281708970665932\n",
      "Iteration #150 loss: 0.09136711061000824\n",
      "Iteration #160 loss: 0.10816890746355057\n",
      "Iteration #170 loss: 0.1020163893699646\n",
      "Iteration #180 loss: 0.17510151863098145\n",
      "Iteration #190 loss: 0.14861561357975006\n",
      "Iteration #200 loss: 0.06421633809804916\n",
      "Iteration #210 loss: 0.0973069965839386\n",
      "Iteration #220 loss: 0.1747499704360962\n",
      "Iteration #230 loss: 0.09875355660915375\n",
      "Iteration #240 loss: 0.18606866896152496\n",
      "Iteration #250 loss: 0.1009691134095192\n",
      "Iteration #260 loss: 0.1564139723777771\n",
      "Iteration #270 loss: 0.050534553825855255\n",
      "Iteration #280 loss: 0.08601024001836777\n",
      "Iteration #290 loss: 0.11171562969684601\n",
      "Iteration #300 loss: 0.06475912034511566\n",
      "Iteration #310 loss: 0.05424090847373009\n",
      "Iteration #320 loss: 0.08867642283439636\n",
      "Iteration #330 loss: 0.0994386076927185\n",
      "Iteration #340 loss: 0.06102326884865761\n",
      "Iteration #350 loss: 0.07456205040216446\n",
      "Iteration #360 loss: 0.09230145066976547\n",
      "Iteration #370 loss: 0.07342282682657242\n",
      "Iteration #380 loss: 0.1766078621149063\n",
      "Iteration #390 loss: 0.15656760334968567\n",
      "Iteration #400 loss: 0.04231905937194824\n",
      "Iteration #410 loss: 0.0798628032207489\n",
      "Iteration #420 loss: 0.06966699659824371\n",
      "Epoch #33 average loss: 0.12161832875317746\n",
      "Epoch #33 accuracy: 0.15188926300037411\n",
      "Iteration #10 loss: 0.13273707032203674\n",
      "Iteration #20 loss: 0.16827771067619324\n",
      "Iteration #30 loss: 0.07970139384269714\n",
      "Iteration #40 loss: 0.11682482808828354\n",
      "Iteration #50 loss: 0.2501980662345886\n",
      "Iteration #60 loss: 0.1520867496728897\n",
      "Iteration #70 loss: 0.12914080917835236\n",
      "Iteration #80 loss: 0.1056070402264595\n",
      "Iteration #90 loss: 0.04881221801042557\n",
      "Iteration #100 loss: 0.1186971664428711\n",
      "Iteration #110 loss: 0.060940928757190704\n",
      "Iteration #120 loss: 0.11768447607755661\n",
      "Iteration #130 loss: 0.12243270874023438\n",
      "Iteration #140 loss: 0.058710966259241104\n",
      "Iteration #150 loss: 0.09146878123283386\n",
      "Iteration #160 loss: 0.1304052621126175\n",
      "Iteration #170 loss: 0.0951443463563919\n",
      "Iteration #180 loss: 0.14523860812187195\n",
      "Iteration #190 loss: 0.1162542924284935\n",
      "Iteration #200 loss: 0.13242612779140472\n",
      "Iteration #210 loss: 0.20362652838230133\n",
      "Iteration #220 loss: 0.028574952855706215\n",
      "Iteration #230 loss: 0.04831841588020325\n",
      "Iteration #240 loss: 0.14352573454380035\n",
      "Iteration #250 loss: 0.08937562257051468\n",
      "Iteration #260 loss: 0.1131935566663742\n",
      "Iteration #270 loss: 0.1109340488910675\n",
      "Iteration #280 loss: 0.14699865877628326\n",
      "Iteration #290 loss: 0.13987407088279724\n",
      "Iteration #300 loss: 0.047874920070171356\n",
      "Iteration #310 loss: 0.07259561866521835\n",
      "Iteration #320 loss: 0.06890945881605148\n",
      "Iteration #330 loss: 0.12936748564243317\n",
      "Iteration #340 loss: 0.08488751947879791\n",
      "Iteration #350 loss: 0.1369296908378601\n",
      "Iteration #360 loss: 0.16491888463497162\n",
      "Iteration #370 loss: 0.1836947649717331\n",
      "Iteration #380 loss: 0.15132786333560944\n",
      "Iteration #390 loss: 0.12379515916109085\n",
      "Iteration #400 loss: 0.19476094841957092\n",
      "Iteration #410 loss: 0.09921663999557495\n",
      "Iteration #420 loss: 0.06484285742044449\n",
      "Epoch #34 average loss: 0.11877142210134843\n",
      "Epoch #34 accuracy: 0.15598123534010946\n",
      "Iteration #10 loss: 0.10888351500034332\n",
      "Iteration #20 loss: 0.05317290499806404\n",
      "Iteration #30 loss: 0.06428143382072449\n",
      "Iteration #40 loss: 0.12403649091720581\n",
      "Iteration #50 loss: 0.0930560976266861\n",
      "Iteration #60 loss: 0.19170834124088287\n",
      "Iteration #70 loss: 0.16467225551605225\n",
      "Iteration #80 loss: 0.13703744113445282\n",
      "Iteration #90 loss: 0.11563806980848312\n",
      "Iteration #100 loss: 0.11538171768188477\n",
      "Iteration #110 loss: 0.09722857922315598\n",
      "Iteration #120 loss: 0.057740770280361176\n",
      "Iteration #130 loss: 0.08114004880189896\n",
      "Iteration #140 loss: 0.11224833875894547\n",
      "Iteration #150 loss: 0.09184928983449936\n",
      "Iteration #160 loss: 0.18697629868984222\n",
      "Iteration #170 loss: 0.06293459981679916\n",
      "Iteration #180 loss: 0.05274587497115135\n",
      "Iteration #190 loss: 0.14342771470546722\n",
      "Iteration #200 loss: 0.18011081218719482\n",
      "Iteration #210 loss: 0.03974773734807968\n",
      "Iteration #220 loss: 0.0897216945886612\n",
      "Iteration #230 loss: 0.11663759499788284\n",
      "Iteration #240 loss: 0.14099383354187012\n",
      "Iteration #250 loss: 0.06754963099956512\n",
      "Iteration #260 loss: 0.10868532210588455\n",
      "Iteration #270 loss: 0.15291807055473328\n",
      "Iteration #280 loss: 0.08321883529424667\n",
      "Iteration #290 loss: 0.057926781475543976\n",
      "Iteration #300 loss: 0.1731446236371994\n",
      "Iteration #310 loss: 0.13980405032634735\n",
      "Iteration #320 loss: 0.15863370895385742\n",
      "Iteration #330 loss: 0.10141121596097946\n",
      "Iteration #340 loss: 0.13874410092830658\n",
      "Iteration #350 loss: 0.2378939390182495\n",
      "Iteration #360 loss: 0.15169228613376617\n",
      "Iteration #370 loss: 0.09539959579706192\n",
      "Iteration #380 loss: 0.02521560713648796\n",
      "Iteration #390 loss: 0.15401121973991394\n",
      "Iteration #400 loss: 0.07319115847349167\n",
      "Iteration #410 loss: 0.17294783890247345\n",
      "Iteration #420 loss: 0.21930743753910065\n",
      "Epoch #35 average loss: 0.11747197871430469\n",
      "Epoch #35 accuracy: 0.14730580825752274\n",
      "Iteration #10 loss: 0.12900109589099884\n",
      "Iteration #20 loss: 0.17041105031967163\n",
      "Iteration #30 loss: 0.17975527048110962\n",
      "Iteration #40 loss: 0.23031575977802277\n",
      "Iteration #50 loss: 0.1958797574043274\n",
      "Iteration #60 loss: 0.0890621766448021\n",
      "Iteration #70 loss: 0.06975551694631577\n",
      "Iteration #80 loss: 0.12626373767852783\n",
      "Iteration #90 loss: 0.0911903977394104\n",
      "Iteration #100 loss: 0.04812031239271164\n",
      "Iteration #110 loss: 0.05454561859369278\n",
      "Iteration #120 loss: 0.1386934220790863\n",
      "Iteration #130 loss: 0.14019156992435455\n",
      "Iteration #140 loss: 0.058663807809352875\n",
      "Iteration #150 loss: 0.11026965081691742\n",
      "Iteration #160 loss: 0.1438668817281723\n",
      "Iteration #170 loss: 0.12286240607500076\n",
      "Iteration #180 loss: 0.143879234790802\n",
      "Iteration #190 loss: 0.1489207148551941\n",
      "Iteration #200 loss: 0.12917256355285645\n",
      "Iteration #210 loss: 0.1838826835155487\n",
      "Iteration #220 loss: 0.0723453164100647\n",
      "Iteration #230 loss: 0.13322502374649048\n",
      "Iteration #240 loss: 0.10444707423448563\n",
      "Iteration #250 loss: 0.07856830954551697\n",
      "Iteration #260 loss: 0.11599300801753998\n",
      "Iteration #270 loss: 0.11291030049324036\n",
      "Iteration #280 loss: 0.06944092363119125\n",
      "Iteration #290 loss: 0.09162851423025131\n",
      "Iteration #300 loss: 0.07192183285951614\n",
      "Iteration #310 loss: 0.08241773396730423\n",
      "Iteration #320 loss: 0.06325700134038925\n",
      "Iteration #330 loss: 0.1253497302532196\n",
      "Iteration #340 loss: 0.0615999698638916\n",
      "Iteration #350 loss: 0.15819038450717926\n",
      "Iteration #360 loss: 0.11176937818527222\n",
      "Iteration #370 loss: 0.11851413547992706\n",
      "Iteration #380 loss: 0.10906803607940674\n",
      "Iteration #390 loss: 0.14259575307369232\n",
      "Iteration #400 loss: 0.07076112926006317\n",
      "Iteration #410 loss: 0.07439074665307999\n",
      "Iteration #420 loss: 0.10831089317798615\n",
      "Epoch #36 average loss: 0.11426387563560185\n",
      "Epoch #36 accuracy: 0.12550231839258114\n",
      "Iteration #10 loss: 0.11662886291742325\n",
      "Iteration #20 loss: 0.264292448759079\n",
      "Iteration #30 loss: 0.07141797244548798\n",
      "Iteration #40 loss: 0.11143144220113754\n",
      "Iteration #50 loss: 0.04743814840912819\n",
      "Iteration #60 loss: 0.031545400619506836\n",
      "Iteration #70 loss: 0.10461433976888657\n",
      "Iteration #80 loss: 0.15020684897899628\n",
      "Iteration #90 loss: 0.16224847733974457\n",
      "Iteration #100 loss: 0.12046726047992706\n",
      "Iteration #110 loss: 0.05996391922235489\n",
      "Iteration #120 loss: 0.11253296583890915\n",
      "Iteration #130 loss: 0.0898834839463234\n",
      "Iteration #140 loss: 0.04504041373729706\n",
      "Iteration #150 loss: 0.11307910084724426\n",
      "Iteration #160 loss: 0.29506635665893555\n",
      "Iteration #170 loss: 0.25139346718788147\n",
      "Iteration #180 loss: 0.09837312251329422\n",
      "Iteration #190 loss: 0.17838814854621887\n",
      "Iteration #200 loss: 0.04669385030865669\n",
      "Iteration #210 loss: 0.19691415131092072\n",
      "Iteration #220 loss: 0.09080557525157928\n",
      "Iteration #230 loss: 0.11515772342681885\n",
      "Iteration #240 loss: 0.18953046202659607\n",
      "Iteration #250 loss: 0.10622233152389526\n",
      "Iteration #260 loss: 0.0649212971329689\n",
      "Iteration #270 loss: 0.1207544207572937\n",
      "Iteration #280 loss: 0.21102501451969147\n",
      "Iteration #290 loss: 0.0845194160938263\n",
      "Iteration #300 loss: 0.12938320636749268\n",
      "Iteration #310 loss: 0.09130983054637909\n",
      "Iteration #320 loss: 0.11515133827924728\n",
      "Iteration #330 loss: 0.04004162177443504\n",
      "Iteration #340 loss: 0.09650954604148865\n",
      "Iteration #350 loss: 0.08154167979955673\n",
      "Iteration #360 loss: 0.1376173198223114\n",
      "Iteration #370 loss: 0.12395237386226654\n",
      "Iteration #380 loss: 0.09763995558023453\n",
      "Iteration #390 loss: 0.1444638967514038\n",
      "Iteration #400 loss: 0.08469416946172714\n",
      "Iteration #410 loss: 0.17624050378799438\n",
      "Iteration #420 loss: 0.2606452405452728\n",
      "Epoch #37 average loss: 0.11271249031263122\n",
      "Epoch #37 accuracy: 0.15860966839792248\n",
      "Iteration #10 loss: 0.15398213267326355\n",
      "Iteration #20 loss: 0.055083613842725754\n",
      "Iteration #30 loss: 0.1495102196931839\n",
      "Iteration #40 loss: 0.10479429364204407\n",
      "Iteration #50 loss: 0.06557321548461914\n",
      "Iteration #60 loss: 0.1500909924507141\n",
      "Iteration #70 loss: 0.08277657628059387\n",
      "Iteration #80 loss: 0.15879704058170319\n",
      "Iteration #90 loss: 0.05753464624285698\n",
      "Iteration #100 loss: 0.11129934340715408\n",
      "Iteration #110 loss: 0.12046300619840622\n",
      "Iteration #120 loss: 0.17683525383472443\n",
      "Iteration #130 loss: 0.11045292764902115\n",
      "Iteration #140 loss: 0.06442242860794067\n",
      "Iteration #150 loss: 0.07162338495254517\n",
      "Iteration #160 loss: 0.07984040677547455\n",
      "Iteration #170 loss: 0.13983877003192902\n",
      "Iteration #180 loss: 0.09441351890563965\n",
      "Iteration #190 loss: 0.07562389969825745\n",
      "Iteration #200 loss: 0.04561936482787132\n",
      "Iteration #210 loss: 0.046467117965221405\n",
      "Iteration #220 loss: 0.2746447026729584\n",
      "Iteration #230 loss: 0.09618955850601196\n",
      "Iteration #240 loss: 0.2249455749988556\n",
      "Iteration #250 loss: 0.14609238505363464\n",
      "Iteration #260 loss: 0.09396287053823471\n",
      "Iteration #270 loss: 0.02629455365240574\n",
      "Iteration #280 loss: 0.10455609112977982\n",
      "Iteration #290 loss: 0.16311568021774292\n",
      "Iteration #300 loss: 0.0458122082054615\n",
      "Iteration #310 loss: 0.10078731179237366\n",
      "Iteration #320 loss: 0.1968987137079239\n",
      "Iteration #330 loss: 0.08057647198438644\n",
      "Iteration #340 loss: 0.12374566495418549\n",
      "Iteration #350 loss: 0.09274972230195999\n",
      "Iteration #360 loss: 0.09573204070329666\n",
      "Iteration #370 loss: 0.05634758248925209\n",
      "Iteration #380 loss: 0.061512503772974014\n",
      "Iteration #390 loss: 0.14034806191921234\n",
      "Iteration #400 loss: 0.046003710478544235\n",
      "Iteration #410 loss: 0.0711817517876625\n",
      "Iteration #420 loss: 0.10652318596839905\n",
      "Epoch #38 average loss: 0.11108525548209912\n",
      "Epoch #38 accuracy: 0.15841161400512382\n",
      "Iteration #10 loss: 0.03454926982522011\n",
      "Iteration #20 loss: 0.20806486904621124\n",
      "Iteration #30 loss: 0.11377866566181183\n",
      "Iteration #40 loss: 0.12733280658721924\n",
      "Iteration #50 loss: 0.1278015673160553\n",
      "Iteration #60 loss: 0.05703013017773628\n",
      "Iteration #70 loss: 0.1818036586046219\n",
      "Iteration #80 loss: 0.05732420086860657\n",
      "Iteration #90 loss: 0.09574000537395477\n",
      "Iteration #100 loss: 0.10305345803499222\n",
      "Iteration #110 loss: 0.12988823652267456\n",
      "Iteration #120 loss: 0.046104103326797485\n",
      "Iteration #130 loss: 0.15242473781108856\n",
      "Iteration #140 loss: 0.08712279796600342\n",
      "Iteration #150 loss: 0.05823433771729469\n",
      "Iteration #160 loss: 0.15430788695812225\n",
      "Iteration #170 loss: 0.0584600567817688\n",
      "Iteration #180 loss: 0.059153392910957336\n",
      "Iteration #190 loss: 0.1843428909778595\n",
      "Iteration #200 loss: 0.15853962302207947\n",
      "Iteration #210 loss: 0.23480938374996185\n",
      "Iteration #220 loss: 0.11880337446928024\n",
      "Iteration #230 loss: 0.05969935283064842\n",
      "Iteration #240 loss: 0.04815283045172691\n",
      "Iteration #250 loss: 0.06693288683891296\n",
      "Iteration #260 loss: 0.10499536991119385\n",
      "Iteration #270 loss: 0.3219432830810547\n",
      "Iteration #280 loss: 0.22328650951385498\n",
      "Iteration #290 loss: 0.14803330600261688\n",
      "Iteration #300 loss: 0.113971047103405\n",
      "Iteration #310 loss: 0.1750907450914383\n",
      "Iteration #320 loss: 0.18145129084587097\n",
      "Iteration #330 loss: 0.06251857429742813\n",
      "Iteration #340 loss: 0.126133531332016\n",
      "Iteration #350 loss: 0.07192058861255646\n",
      "Iteration #360 loss: 0.11266651004552841\n",
      "Iteration #370 loss: 0.19985273480415344\n",
      "Iteration #380 loss: 0.09319611638784409\n",
      "Iteration #390 loss: 0.03728261590003967\n",
      "Iteration #400 loss: 0.19181029498577118\n",
      "Iteration #410 loss: 0.11876671761274338\n",
      "Iteration #420 loss: 0.10527920722961426\n",
      "Epoch #39 average loss: 0.109863955500998\n",
      "Epoch #39 accuracy: 0.16966786714685875\n",
      "Iteration #10 loss: 0.11791474372148514\n",
      "Iteration #20 loss: 0.07955031096935272\n",
      "Iteration #30 loss: 0.06310752034187317\n",
      "Iteration #40 loss: 0.10543234646320343\n",
      "Iteration #50 loss: 0.12050295621156693\n",
      "Iteration #60 loss: 0.06855118274688721\n",
      "Iteration #70 loss: 0.08552689105272293\n",
      "Iteration #80 loss: 0.14476926624774933\n",
      "Iteration #90 loss: 0.18009084463119507\n",
      "Iteration #100 loss: 0.12377658486366272\n",
      "Iteration #110 loss: 0.11671897023916245\n",
      "Iteration #120 loss: 0.14903603494167328\n",
      "Iteration #130 loss: 0.07265079021453857\n",
      "Iteration #140 loss: 0.07255624234676361\n",
      "Iteration #150 loss: 0.10038755089044571\n",
      "Iteration #160 loss: 0.05027453601360321\n",
      "Iteration #170 loss: 0.11456665396690369\n",
      "Iteration #180 loss: 0.09290558099746704\n",
      "Iteration #190 loss: 0.047648318111896515\n",
      "Iteration #200 loss: 0.12316976487636566\n",
      "Iteration #210 loss: 0.09415461122989655\n",
      "Iteration #220 loss: 0.04881400242447853\n",
      "Iteration #230 loss: 0.054753027856349945\n",
      "Iteration #240 loss: 0.1320040374994278\n",
      "Iteration #250 loss: 0.041844479739665985\n",
      "Iteration #260 loss: 0.04868725687265396\n",
      "Iteration #270 loss: 0.06692260503768921\n",
      "Iteration #280 loss: 0.027497440576553345\n",
      "Iteration #290 loss: 0.09356467425823212\n",
      "Iteration #300 loss: 0.11014898121356964\n",
      "Iteration #310 loss: 0.09763975441455841\n",
      "Iteration #320 loss: 0.10320871323347092\n",
      "Iteration #330 loss: 0.1427721530199051\n",
      "Iteration #340 loss: 0.07265528291463852\n",
      "Iteration #350 loss: 0.09064716100692749\n",
      "Iteration #360 loss: 0.12323835492134094\n",
      "Iteration #370 loss: 0.07036424428224564\n",
      "Iteration #380 loss: 0.12646900117397308\n",
      "Iteration #390 loss: 0.0967443585395813\n",
      "Iteration #400 loss: 0.08623459935188293\n",
      "Iteration #410 loss: 0.15075962245464325\n",
      "Iteration #420 loss: 0.1446576565504074\n",
      "Epoch #40 average loss: 0.1093085911036458\n",
      "Epoch #40 accuracy: 0.14734022196708763\n",
      "Iteration #10 loss: 0.15523676574230194\n",
      "Iteration #20 loss: 0.053224969655275345\n",
      "Iteration #30 loss: 0.19089698791503906\n",
      "Iteration #40 loss: 0.0878986120223999\n",
      "Iteration #50 loss: 0.1418742835521698\n",
      "Iteration #60 loss: 0.08026517182588577\n",
      "Iteration #70 loss: 0.10193277895450592\n",
      "Iteration #80 loss: 0.0945909172296524\n",
      "Iteration #90 loss: 0.06006229296326637\n",
      "Iteration #100 loss: 0.058491695672273636\n",
      "Iteration #110 loss: 0.029211658984422684\n",
      "Iteration #120 loss: 0.034220077097415924\n",
      "Iteration #130 loss: 0.0724196657538414\n",
      "Iteration #140 loss: 0.11591151356697083\n",
      "Iteration #150 loss: 0.07549530267715454\n",
      "Iteration #160 loss: 0.06470971554517746\n",
      "Iteration #170 loss: 0.08670075237751007\n",
      "Iteration #180 loss: 0.08064816147089005\n",
      "Iteration #190 loss: 0.07124155014753342\n",
      "Iteration #200 loss: 0.07415348291397095\n",
      "Iteration #210 loss: 0.07905277609825134\n",
      "Iteration #220 loss: 0.1821843534708023\n",
      "Iteration #230 loss: 0.11009030044078827\n",
      "Iteration #240 loss: 0.11081688106060028\n",
      "Iteration #250 loss: 0.29894039034843445\n",
      "Iteration #260 loss: 0.04649557173252106\n",
      "Iteration #270 loss: 0.10050316900014877\n",
      "Iteration #280 loss: 0.05580202117562294\n",
      "Iteration #290 loss: 0.07112175226211548\n",
      "Iteration #300 loss: 0.05493435263633728\n",
      "Iteration #310 loss: 0.11003396660089493\n",
      "Iteration #320 loss: 0.07538831979036331\n",
      "Iteration #330 loss: 0.19908948242664337\n",
      "Iteration #340 loss: 0.32581406831741333\n",
      "Iteration #350 loss: 0.04967215657234192\n",
      "Iteration #360 loss: 0.09778831899166107\n",
      "Iteration #370 loss: 0.09403601288795471\n",
      "Iteration #380 loss: 0.1361020803451538\n",
      "Iteration #390 loss: 0.036598090082407\n",
      "Iteration #400 loss: 0.16575363278388977\n",
      "Iteration #410 loss: 0.050144560635089874\n",
      "Iteration #420 loss: 0.06275418400764465\n",
      "Epoch #41 average loss: 0.10846632003624904\n",
      "Epoch #41 accuracy: 0.16554721453970764\n",
      "Iteration #10 loss: 0.09688881784677505\n",
      "Iteration #20 loss: 0.060807835310697556\n",
      "Iteration #30 loss: 0.05702577903866768\n",
      "Iteration #40 loss: 0.20617786049842834\n",
      "Iteration #50 loss: 0.16174882650375366\n",
      "Iteration #60 loss: 0.09352491050958633\n",
      "Iteration #70 loss: 0.05161847174167633\n",
      "Iteration #80 loss: 0.1515563279390335\n",
      "Iteration #90 loss: 0.21454523503780365\n",
      "Iteration #100 loss: 0.09746766090393066\n",
      "Iteration #110 loss: 0.12011348456144333\n",
      "Iteration #120 loss: 0.05022367089986801\n",
      "Iteration #130 loss: 0.06903630495071411\n",
      "Iteration #140 loss: 0.0725337564945221\n",
      "Iteration #150 loss: 0.13353002071380615\n",
      "Iteration #160 loss: 0.13402678072452545\n",
      "Iteration #170 loss: 0.11775203049182892\n",
      "Iteration #180 loss: 0.11496582627296448\n",
      "Iteration #190 loss: 0.0818786770105362\n",
      "Iteration #200 loss: 0.22714705765247345\n",
      "Iteration #210 loss: 0.16084468364715576\n",
      "Iteration #220 loss: 0.09345291554927826\n",
      "Iteration #230 loss: 0.05085998401045799\n",
      "Iteration #240 loss: 0.11507586389780045\n",
      "Iteration #250 loss: 0.09989558160305023\n",
      "Iteration #260 loss: 0.1005510613322258\n",
      "Iteration #270 loss: 0.184615820646286\n",
      "Iteration #280 loss: 0.06283720582723618\n",
      "Iteration #290 loss: 0.07844511419534683\n",
      "Iteration #300 loss: 0.08646867424249649\n",
      "Iteration #310 loss: 0.03115854412317276\n",
      "Iteration #320 loss: 0.11977814882993698\n",
      "Iteration #330 loss: 0.3386864960193634\n",
      "Iteration #340 loss: 0.057663694024086\n",
      "Iteration #350 loss: 0.12252571433782578\n",
      "Iteration #360 loss: 0.10414862632751465\n",
      "Iteration #370 loss: 0.08340935409069061\n",
      "Iteration #380 loss: 0.0760025903582573\n",
      "Iteration #390 loss: 0.11480271071195602\n",
      "Iteration #400 loss: 0.1275840401649475\n",
      "Iteration #410 loss: 0.1511697620153427\n",
      "Iteration #420 loss: 0.11593423783779144\n",
      "Epoch #42 average loss: 0.10492714838680066\n",
      "Epoch #42 accuracy: 0.16809760506100316\n",
      "Iteration #10 loss: 0.04249446466565132\n",
      "Iteration #20 loss: 0.07406213134527206\n",
      "Iteration #30 loss: 0.10225651413202286\n",
      "Iteration #40 loss: 0.1917617917060852\n",
      "Iteration #50 loss: 0.08913618326187134\n",
      "Iteration #60 loss: 0.1794929951429367\n",
      "Iteration #70 loss: 0.10622164607048035\n",
      "Iteration #80 loss: 0.060410644859075546\n",
      "Iteration #90 loss: 0.14417904615402222\n",
      "Iteration #100 loss: 0.19412779808044434\n",
      "Iteration #110 loss: 0.07545731961727142\n",
      "Iteration #120 loss: 0.04513511806726456\n",
      "Iteration #130 loss: 0.10473078489303589\n",
      "Iteration #140 loss: 0.03898775577545166\n",
      "Iteration #150 loss: 0.06454242765903473\n",
      "Iteration #160 loss: 0.19143234193325043\n",
      "Iteration #170 loss: 0.040957577526569366\n",
      "Iteration #180 loss: 0.09404130280017853\n",
      "Iteration #190 loss: 0.05188261717557907\n",
      "Iteration #200 loss: 0.1967032104730606\n",
      "Iteration #210 loss: 0.10148013383150101\n",
      "Iteration #220 loss: 0.24678489565849304\n",
      "Iteration #230 loss: 0.08772702515125275\n",
      "Iteration #240 loss: 0.055998753756284714\n",
      "Iteration #250 loss: 0.06557659804821014\n",
      "Iteration #260 loss: 0.05904100835323334\n",
      "Iteration #270 loss: 0.08635342866182327\n",
      "Iteration #280 loss: 0.12041902542114258\n",
      "Iteration #290 loss: 0.17782209813594818\n",
      "Iteration #300 loss: 0.07206912338733673\n",
      "Iteration #310 loss: 0.1061665341258049\n",
      "Iteration #320 loss: 0.04252628609538078\n",
      "Iteration #330 loss: 0.04973714053630829\n",
      "Iteration #340 loss: 0.08442962914705276\n",
      "Iteration #350 loss: 0.1569012552499771\n",
      "Iteration #360 loss: 0.11063367873430252\n",
      "Iteration #370 loss: 0.04899463430047035\n",
      "Iteration #380 loss: 0.13993802666664124\n",
      "Iteration #390 loss: 0.056233372539281845\n",
      "Iteration #400 loss: 0.1865977644920349\n",
      "Iteration #410 loss: 0.07207119464874268\n",
      "Iteration #420 loss: 0.13986819982528687\n",
      "Epoch #43 average loss: 0.10394749185623438\n",
      "Epoch #43 accuracy: 0.17015170151701517\n",
      "Iteration #10 loss: 0.11505359411239624\n",
      "Iteration #20 loss: 0.10411510616540909\n",
      "Iteration #30 loss: 0.08700937032699585\n",
      "Iteration #40 loss: 0.07439246028661728\n",
      "Iteration #50 loss: 0.138075053691864\n",
      "Iteration #60 loss: 0.03282777965068817\n",
      "Iteration #70 loss: 0.15966349840164185\n",
      "Iteration #80 loss: 0.06418476998806\n",
      "Iteration #90 loss: 0.0632094144821167\n",
      "Iteration #100 loss: 0.09573172777891159\n",
      "Iteration #110 loss: 0.04364620894193649\n",
      "Iteration #120 loss: 0.1364421397447586\n",
      "Iteration #130 loss: 0.0655527114868164\n",
      "Iteration #140 loss: 0.05605750530958176\n",
      "Iteration #150 loss: 0.13891589641571045\n",
      "Iteration #160 loss: 0.10168386995792389\n",
      "Iteration #170 loss: 0.06722714006900787\n",
      "Iteration #180 loss: 0.061390649527311325\n",
      "Iteration #190 loss: 0.06026560440659523\n",
      "Iteration #200 loss: 0.09252244234085083\n",
      "Iteration #210 loss: 0.07135336101055145\n",
      "Iteration #220 loss: 0.16392165422439575\n",
      "Iteration #230 loss: 0.14286521077156067\n",
      "Iteration #240 loss: 0.14373736083507538\n",
      "Iteration #250 loss: 0.12402975559234619\n",
      "Iteration #260 loss: 0.1456020027399063\n",
      "Iteration #270 loss: 0.13433237373828888\n",
      "Iteration #280 loss: 0.20421066880226135\n",
      "Iteration #290 loss: 0.11061991751194\n",
      "Iteration #300 loss: 0.06813859939575195\n",
      "Iteration #310 loss: 0.097347691655159\n",
      "Iteration #320 loss: 0.11529885977506638\n",
      "Iteration #330 loss: 0.12477117031812668\n",
      "Iteration #340 loss: 0.08493738621473312\n",
      "Iteration #350 loss: 0.021799150854349136\n",
      "Iteration #360 loss: 0.11381149291992188\n",
      "Iteration #370 loss: 0.05503816902637482\n",
      "Iteration #380 loss: 0.059577394276857376\n",
      "Iteration #390 loss: 0.2779727876186371\n",
      "Iteration #400 loss: 0.12522312998771667\n",
      "Iteration #410 loss: 0.09003813564777374\n",
      "Iteration #420 loss: 0.09257784485816956\n",
      "Epoch #44 average loss: 0.1015907539775918\n",
      "Epoch #44 accuracy: 0.15465861938890985\n",
      "Iteration #10 loss: 0.09002143889665604\n",
      "Iteration #20 loss: 0.10857877880334854\n",
      "Iteration #30 loss: 0.1171373501420021\n",
      "Iteration #40 loss: 0.20191828906536102\n",
      "Iteration #50 loss: 0.07271993160247803\n",
      "Iteration #60 loss: 0.15497350692749023\n",
      "Iteration #70 loss: 0.06949566304683685\n",
      "Iteration #80 loss: 0.057497862726449966\n",
      "Iteration #90 loss: 0.08688460290431976\n",
      "Iteration #100 loss: 0.05926830694079399\n",
      "Iteration #110 loss: 0.12465249001979828\n",
      "Iteration #120 loss: 0.08538378030061722\n",
      "Iteration #130 loss: 0.05614852160215378\n",
      "Iteration #140 loss: 0.15961840748786926\n",
      "Iteration #150 loss: 0.06431762874126434\n",
      "Iteration #160 loss: 0.1751677691936493\n",
      "Iteration #170 loss: 0.11905577778816223\n",
      "Iteration #180 loss: 0.15167824923992157\n",
      "Iteration #190 loss: 0.10440242290496826\n",
      "Iteration #200 loss: 0.06019623950123787\n",
      "Iteration #210 loss: 0.059426479041576385\n",
      "Iteration #220 loss: 0.04286615177989006\n",
      "Iteration #230 loss: 0.06934139132499695\n",
      "Iteration #240 loss: 0.08803345263004303\n",
      "Iteration #250 loss: 0.022003697231411934\n",
      "Iteration #260 loss: 0.05047839507460594\n",
      "Iteration #270 loss: 0.049540791660547256\n",
      "Iteration #280 loss: 0.03472946211695671\n",
      "Iteration #290 loss: 0.03804825246334076\n",
      "Iteration #300 loss: 0.08386337012052536\n",
      "Iteration #310 loss: 0.07948072999715805\n",
      "Iteration #320 loss: 0.1677251160144806\n",
      "Iteration #330 loss: 0.11788300424814224\n",
      "Iteration #340 loss: 0.14704057574272156\n",
      "Iteration #350 loss: 0.0895935446023941\n",
      "Iteration #360 loss: 0.07021813839673996\n",
      "Iteration #370 loss: 0.05373537540435791\n",
      "Iteration #380 loss: 0.03292809799313545\n",
      "Iteration #390 loss: 0.06865859031677246\n",
      "Iteration #400 loss: 0.12673118710517883\n",
      "Iteration #410 loss: 0.10302388668060303\n",
      "Iteration #420 loss: 0.10812455415725708\n",
      "Epoch #45 average loss: 0.10087697192442531\n",
      "Epoch #45 accuracy: 0.17089160839160839\n",
      "Iteration #10 loss: 0.05356450006365776\n",
      "Iteration #20 loss: 0.13348807394504547\n",
      "Iteration #30 loss: 0.053211335092782974\n",
      "Iteration #40 loss: 0.06864961236715317\n",
      "Iteration #50 loss: 0.13792261481285095\n",
      "Iteration #60 loss: 0.07434616982936859\n",
      "Iteration #70 loss: 0.16350753605365753\n",
      "Iteration #80 loss: 0.04960368201136589\n",
      "Iteration #90 loss: 0.07221244275569916\n",
      "Iteration #100 loss: 0.06068815663456917\n",
      "Iteration #110 loss: 0.05468147248029709\n",
      "Iteration #120 loss: 0.12620709836483002\n",
      "Iteration #130 loss: 0.107112817466259\n",
      "Iteration #140 loss: 0.10388384014368057\n",
      "Iteration #150 loss: 0.07492626458406448\n",
      "Iteration #160 loss: 0.14610235393047333\n",
      "Iteration #170 loss: 0.10407131910324097\n",
      "Iteration #180 loss: 0.11320527642965317\n",
      "Iteration #190 loss: 0.09046106040477753\n",
      "Iteration #200 loss: 0.10263022035360336\n",
      "Iteration #210 loss: 0.15624666213989258\n",
      "Iteration #220 loss: 0.1609126776456833\n",
      "Iteration #230 loss: 0.06602130830287933\n",
      "Iteration #240 loss: 0.028996281325817108\n",
      "Iteration #250 loss: 0.033709969371557236\n",
      "Iteration #260 loss: 0.0734623372554779\n",
      "Iteration #270 loss: 0.11822153627872467\n",
      "Iteration #280 loss: 0.18447476625442505\n",
      "Iteration #290 loss: 0.15884996950626373\n",
      "Iteration #300 loss: 0.07274776697158813\n",
      "Iteration #310 loss: 0.08594197779893875\n",
      "Iteration #320 loss: 0.06969492137432098\n",
      "Iteration #330 loss: 0.20889581739902496\n",
      "Iteration #340 loss: 0.07996489107608795\n",
      "Iteration #350 loss: 0.08372078090906143\n",
      "Iteration #360 loss: 0.10772910714149475\n",
      "Iteration #370 loss: 0.13198354840278625\n",
      "Iteration #380 loss: 0.0645473524928093\n",
      "Iteration #390 loss: 0.0657830461859703\n",
      "Iteration #400 loss: 0.13563942909240723\n",
      "Iteration #410 loss: 0.11878399550914764\n",
      "Iteration #420 loss: 0.07841157168149948\n",
      "Epoch #46 average loss: 0.0982754561837736\n",
      "Epoch #46 accuracy: 0.1788702928870293\n",
      "Iteration #10 loss: 0.05559016391634941\n",
      "Iteration #20 loss: 0.058830950409173965\n",
      "Iteration #30 loss: 0.07141895592212677\n",
      "Iteration #40 loss: 0.1713176816701889\n",
      "Iteration #50 loss: 0.07956980168819427\n",
      "Iteration #60 loss: 0.031100371852517128\n",
      "Iteration #70 loss: 0.13625332713127136\n",
      "Iteration #80 loss: 0.10938514769077301\n",
      "Iteration #90 loss: 0.056067973375320435\n",
      "Iteration #100 loss: 0.09505730122327805\n",
      "Iteration #110 loss: 0.05271190404891968\n",
      "Iteration #120 loss: 0.10234814882278442\n",
      "Iteration #130 loss: 0.10440604388713837\n",
      "Iteration #140 loss: 0.1536429077386856\n",
      "Iteration #150 loss: 0.06772679835557938\n",
      "Iteration #160 loss: 0.10077445954084396\n",
      "Iteration #170 loss: 0.12723667919635773\n",
      "Iteration #180 loss: 0.14798125624656677\n",
      "Iteration #190 loss: 0.1357842981815338\n",
      "Iteration #200 loss: 0.07693924009799957\n",
      "Iteration #210 loss: 0.25929099321365356\n",
      "Iteration #220 loss: 0.05837640166282654\n",
      "Iteration #230 loss: 0.04194566234946251\n",
      "Iteration #240 loss: 0.07262668013572693\n",
      "Iteration #250 loss: 0.04245496168732643\n",
      "Iteration #260 loss: 0.06492293626070023\n",
      "Iteration #270 loss: 0.09333111345767975\n",
      "Iteration #280 loss: 0.034381523728370667\n",
      "Iteration #290 loss: 0.14388638734817505\n",
      "Iteration #300 loss: 0.1352922022342682\n",
      "Iteration #310 loss: 0.08051624894142151\n",
      "Iteration #320 loss: 0.11514054238796234\n",
      "Iteration #330 loss: 0.12800277769565582\n",
      "Iteration #340 loss: 0.0843970775604248\n",
      "Iteration #350 loss: 0.0960865169763565\n",
      "Iteration #360 loss: 0.13826894760131836\n",
      "Iteration #370 loss: 0.1420443058013916\n",
      "Iteration #380 loss: 0.06312777101993561\n",
      "Iteration #390 loss: 0.08628054708242416\n",
      "Iteration #400 loss: 0.08060456067323685\n",
      "Iteration #410 loss: 0.19140365719795227\n",
      "Iteration #420 loss: 0.03378341346979141\n",
      "Epoch #47 average loss: 0.09764270587092058\n",
      "Epoch #47 accuracy: 0.16615519508987286\n",
      "Iteration #10 loss: 0.027395326644182205\n",
      "Iteration #20 loss: 0.10215392708778381\n",
      "Iteration #30 loss: 0.10236430168151855\n",
      "Iteration #40 loss: 0.12826934456825256\n",
      "Iteration #50 loss: 0.09483027458190918\n",
      "Iteration #60 loss: 0.11670853942632675\n",
      "Iteration #70 loss: 0.09283875674009323\n",
      "Iteration #80 loss: 0.061159852892160416\n",
      "Iteration #90 loss: 0.09606014937162399\n",
      "Iteration #100 loss: 0.07080331444740295\n",
      "Iteration #110 loss: 0.046487919986248016\n",
      "Iteration #120 loss: 0.06947391480207443\n",
      "Iteration #130 loss: 0.11293841153383255\n",
      "Iteration #140 loss: 0.09023440629243851\n",
      "Iteration #150 loss: 0.023401571437716484\n",
      "Iteration #160 loss: 0.07415520399808884\n",
      "Iteration #170 loss: 0.0943940058350563\n",
      "Iteration #180 loss: 0.20761451125144958\n",
      "Iteration #190 loss: 0.09324911236763\n",
      "Iteration #200 loss: 0.052745990455150604\n",
      "Iteration #210 loss: 0.042137157171964645\n",
      "Iteration #220 loss: 0.12112418562173843\n",
      "Iteration #230 loss: 0.06000674515962601\n",
      "Iteration #240 loss: 0.07613662630319595\n",
      "Iteration #250 loss: 0.05494829639792442\n",
      "Iteration #260 loss: 0.08375698328018188\n",
      "Iteration #270 loss: 0.1913679838180542\n",
      "Iteration #280 loss: 0.05621199682354927\n",
      "Iteration #290 loss: 0.0982225239276886\n",
      "Iteration #300 loss: 0.05554936081171036\n",
      "Iteration #310 loss: 0.0574522539973259\n",
      "Iteration #320 loss: 0.088950976729393\n",
      "Iteration #330 loss: 0.12819795310497284\n",
      "Iteration #340 loss: 0.16187803447246552\n",
      "Iteration #350 loss: 0.02723570354282856\n",
      "Iteration #360 loss: 0.19285528361797333\n",
      "Iteration #370 loss: 0.08910965919494629\n",
      "Iteration #380 loss: 0.026472005993127823\n",
      "Iteration #390 loss: 0.058451563119888306\n",
      "Iteration #400 loss: 0.19676722586154938\n",
      "Iteration #410 loss: 0.058428674936294556\n",
      "Iteration #420 loss: 0.11645151674747467\n",
      "Epoch #48 average loss: 0.093847725044874\n",
      "Epoch #48 accuracy: 0.1763901549680948\n",
      "Iteration #10 loss: 0.06223390996456146\n",
      "Iteration #20 loss: 0.09141363203525543\n",
      "Iteration #30 loss: 0.14493536949157715\n",
      "Iteration #40 loss: 0.11149843782186508\n",
      "Iteration #50 loss: 0.08098552376031876\n",
      "Iteration #60 loss: 0.10082218050956726\n",
      "Iteration #70 loss: 0.04916636645793915\n",
      "Iteration #80 loss: 0.102214515209198\n",
      "Iteration #90 loss: 0.03737623244524002\n",
      "Iteration #100 loss: 0.11539614200592041\n",
      "Iteration #110 loss: 0.06807375699281693\n",
      "Iteration #120 loss: 0.0440649650990963\n",
      "Iteration #130 loss: 0.12056805938482285\n",
      "Iteration #140 loss: 0.12944059073925018\n",
      "Iteration #150 loss: 0.08412764221429825\n",
      "Iteration #160 loss: 0.0576384961605072\n",
      "Iteration #170 loss: 0.08554346114397049\n",
      "Iteration #180 loss: 0.05600523576140404\n",
      "Iteration #190 loss: 0.06489089131355286\n",
      "Iteration #200 loss: 0.1017225906252861\n",
      "Iteration #210 loss: 0.09363480657339096\n",
      "Iteration #220 loss: 0.031106431037187576\n",
      "Iteration #230 loss: 0.1004628837108612\n",
      "Iteration #240 loss: 0.06364187598228455\n",
      "Iteration #250 loss: 0.09683068096637726\n",
      "Iteration #260 loss: 0.06503318250179291\n",
      "Iteration #270 loss: 0.06231103837490082\n",
      "Iteration #280 loss: 0.0962330773472786\n",
      "Iteration #290 loss: 0.3059636652469635\n",
      "Iteration #300 loss: 0.1474226415157318\n",
      "Iteration #310 loss: 0.15807537734508514\n",
      "Iteration #320 loss: 0.10177698731422424\n",
      "Iteration #330 loss: 0.11941222846508026\n",
      "Iteration #340 loss: 0.10578662902116776\n",
      "Iteration #350 loss: 0.046918127685785294\n",
      "Iteration #360 loss: 0.1339382529258728\n",
      "Iteration #370 loss: 0.10971862822771072\n",
      "Iteration #380 loss: 0.09799467772245407\n",
      "Iteration #390 loss: 0.12679323554039001\n",
      "Iteration #400 loss: 0.09980471432209015\n",
      "Iteration #410 loss: 0.11769703030586243\n",
      "Iteration #420 loss: 0.08352750539779663\n",
      "Epoch #49 average loss: 0.0946390327738682\n",
      "Epoch #49 accuracy: 0.1829330608073582\n",
      "Iteration #10 loss: 0.06772208958864212\n",
      "Iteration #20 loss: 0.15834440290927887\n",
      "Iteration #30 loss: 0.1699245721101761\n",
      "Iteration #40 loss: 0.161128968000412\n",
      "Iteration #50 loss: 0.06111200153827667\n",
      "Iteration #60 loss: 0.15998929738998413\n",
      "Iteration #70 loss: 0.08010847121477127\n",
      "Iteration #80 loss: 0.13941043615341187\n",
      "Iteration #90 loss: 0.1646709144115448\n",
      "Iteration #100 loss: 0.09979373961687088\n",
      "Iteration #110 loss: 0.15719269216060638\n",
      "Iteration #120 loss: 0.09670546650886536\n",
      "Iteration #130 loss: 0.05594683066010475\n",
      "Iteration #140 loss: 0.17992040514945984\n",
      "Iteration #150 loss: 0.05352770537137985\n",
      "Iteration #160 loss: 0.04183457791805267\n",
      "Iteration #170 loss: 0.19402264058589935\n",
      "Iteration #180 loss: 0.07228546589612961\n",
      "Iteration #190 loss: 0.11359121650457382\n",
      "Iteration #200 loss: 0.11957883834838867\n",
      "Iteration #210 loss: 0.08047636598348618\n",
      "Iteration #220 loss: 0.08573394268751144\n",
      "Iteration #230 loss: 0.11022830754518509\n",
      "Iteration #240 loss: 0.12644515931606293\n",
      "Iteration #250 loss: 0.11063408851623535\n",
      "Iteration #260 loss: 0.053910475224256516\n",
      "Iteration #270 loss: 0.029877927154302597\n",
      "Iteration #280 loss: 0.07824716717004776\n",
      "Iteration #290 loss: 0.05756742134690285\n",
      "Iteration #300 loss: 0.04324056953191757\n",
      "Iteration #310 loss: 0.0493132546544075\n",
      "Iteration #320 loss: 0.06782504171133041\n",
      "Iteration #330 loss: 0.14978715777397156\n",
      "Iteration #340 loss: 0.049789462238550186\n",
      "Iteration #350 loss: 0.15838757157325745\n",
      "Iteration #360 loss: 0.07191478461027145\n",
      "Iteration #370 loss: 0.07202969491481781\n",
      "Iteration #380 loss: 0.129679337143898\n",
      "Iteration #390 loss: 0.0658554807305336\n",
      "Iteration #400 loss: 0.06630008667707443\n",
      "Iteration #410 loss: 0.06735599786043167\n",
      "Iteration #420 loss: 0.1973332166671753\n",
      "Epoch #50 average loss: 0.0945499121895029\n",
      "Epoch #50 accuracy: 0.18960244648318042\n",
      "Iteration #10 loss: 0.13123996555805206\n",
      "Iteration #20 loss: 0.11209212243556976\n",
      "Iteration #30 loss: 0.0831097811460495\n",
      "Iteration #40 loss: 0.15248249471187592\n",
      "Iteration #50 loss: 0.10306121408939362\n",
      "Iteration #60 loss: 0.1527281403541565\n",
      "Iteration #70 loss: 0.11836227029561996\n",
      "Iteration #80 loss: 0.05016964673995972\n",
      "Iteration #90 loss: 0.05729822441935539\n",
      "Iteration #100 loss: 0.04895960912108421\n",
      "Iteration #110 loss: 0.14869460463523865\n",
      "Iteration #120 loss: 0.12653416395187378\n",
      "Iteration #130 loss: 0.09306050837039948\n",
      "Iteration #140 loss: 0.08579115569591522\n",
      "Iteration #150 loss: 0.10041838884353638\n",
      "Iteration #160 loss: 0.09217198938131332\n",
      "Iteration #170 loss: 0.06814011186361313\n",
      "Iteration #180 loss: 0.08361005038022995\n",
      "Iteration #190 loss: 0.20150570571422577\n",
      "Iteration #200 loss: 0.18107104301452637\n",
      "Iteration #210 loss: 0.22354274988174438\n",
      "Iteration #220 loss: 0.06658821552991867\n",
      "Iteration #230 loss: 0.047697123140096664\n",
      "Iteration #240 loss: 0.04782436415553093\n",
      "Iteration #250 loss: 0.029356511309742928\n",
      "Iteration #260 loss: 0.11627598106861115\n",
      "Iteration #270 loss: 0.12314717471599579\n",
      "Iteration #280 loss: 0.029371753334999084\n",
      "Iteration #290 loss: 0.06176960840821266\n",
      "Iteration #300 loss: 0.0973752960562706\n",
      "Iteration #310 loss: 0.028089575469493866\n",
      "Iteration #320 loss: 0.03800245374441147\n",
      "Iteration #330 loss: 0.09635291248559952\n",
      "Iteration #340 loss: 0.06117600202560425\n",
      "Iteration #350 loss: 0.09933739900588989\n",
      "Iteration #360 loss: 0.10877259075641632\n",
      "Iteration #370 loss: 0.03524654358625412\n",
      "Iteration #380 loss: 0.07462027668952942\n",
      "Iteration #390 loss: 0.09079114347696304\n",
      "Iteration #400 loss: 0.09663856774568558\n",
      "Iteration #410 loss: 0.08465547859668732\n",
      "Iteration #420 loss: 0.04540189728140831\n",
      "Epoch #51 average loss: 0.09534365583804044\n",
      "Epoch #51 accuracy: 0.18128654970760233\n",
      "Iteration #10 loss: 0.03898821398615837\n",
      "Iteration #20 loss: 0.04271406680345535\n",
      "Iteration #30 loss: 0.0974704697728157\n",
      "Iteration #40 loss: 0.03425866737961769\n",
      "Iteration #50 loss: 0.09242503345012665\n",
      "Iteration #60 loss: 0.1312911957502365\n",
      "Iteration #70 loss: 0.03998364508152008\n",
      "Iteration #80 loss: 0.029428819194436073\n",
      "Iteration #90 loss: 0.12131062895059586\n",
      "Iteration #100 loss: 0.09462778270244598\n",
      "Iteration #110 loss: 0.09470139443874359\n",
      "Iteration #120 loss: 0.06502192467451096\n",
      "Iteration #130 loss: 0.15613432228565216\n",
      "Iteration #140 loss: 0.09715144336223602\n",
      "Iteration #150 loss: 0.07291065901517868\n",
      "Iteration #160 loss: 0.19404716789722443\n",
      "Iteration #170 loss: 0.11589380353689194\n",
      "Iteration #180 loss: 0.10979726910591125\n",
      "Iteration #190 loss: 0.12275946140289307\n",
      "Iteration #200 loss: 0.181911438703537\n",
      "Iteration #210 loss: 0.07271601259708405\n",
      "Iteration #220 loss: 0.04498939961194992\n",
      "Iteration #230 loss: 0.16763189435005188\n",
      "Iteration #240 loss: 0.06305811554193497\n",
      "Iteration #250 loss: 0.03382014110684395\n",
      "Iteration #260 loss: 0.07328411936759949\n",
      "Iteration #270 loss: 0.09690672159194946\n",
      "Iteration #280 loss: 0.10903909057378769\n",
      "Iteration #290 loss: 0.1750490665435791\n",
      "Iteration #300 loss: 0.06332466006278992\n",
      "Iteration #310 loss: 0.061370231211185455\n",
      "Iteration #320 loss: 0.10480047762393951\n",
      "Iteration #330 loss: 0.1527528613805771\n",
      "Iteration #340 loss: 0.030560994520783424\n",
      "Iteration #350 loss: 0.10842789709568024\n",
      "Iteration #360 loss: 0.0541544072329998\n",
      "Iteration #370 loss: 0.12278646975755692\n",
      "Iteration #380 loss: 0.07687968760728836\n",
      "Iteration #390 loss: 0.0640791729092598\n",
      "Iteration #400 loss: 0.037115126848220825\n",
      "Iteration #410 loss: 0.14263014495372772\n",
      "Iteration #420 loss: 0.021548643708229065\n",
      "Epoch #52 average loss: 0.09268489502140292\n",
      "Epoch #52 accuracy: 0.17209971236816873\n",
      "Iteration #10 loss: 0.16164585947990417\n",
      "Iteration #20 loss: 0.12141991406679153\n",
      "Iteration #30 loss: 0.06174148991703987\n",
      "Iteration #40 loss: 0.0852903202176094\n",
      "Iteration #50 loss: 0.052024323493242264\n",
      "Iteration #60 loss: 0.051079727709293365\n",
      "Iteration #70 loss: 0.049772959202528\n",
      "Iteration #80 loss: 0.040959130972623825\n",
      "Iteration #90 loss: 0.0803915411233902\n",
      "Iteration #100 loss: 0.04017987102270126\n",
      "Iteration #110 loss: 0.07300729304552078\n",
      "Iteration #120 loss: 0.07181908190250397\n",
      "Iteration #130 loss: 0.08989954739809036\n",
      "Iteration #140 loss: 0.11683060228824615\n",
      "Iteration #150 loss: 0.0431731641292572\n",
      "Iteration #160 loss: 0.07814580947160721\n",
      "Iteration #170 loss: 0.04587826132774353\n",
      "Iteration #180 loss: 0.0545656718313694\n",
      "Iteration #190 loss: 0.10872910916805267\n",
      "Iteration #200 loss: 0.098899707198143\n",
      "Iteration #210 loss: 0.1332383155822754\n",
      "Iteration #220 loss: 0.225041925907135\n",
      "Iteration #230 loss: 0.06544703245162964\n",
      "Iteration #240 loss: 0.08442217111587524\n",
      "Iteration #250 loss: 0.06839285790920258\n",
      "Iteration #260 loss: 0.23016688227653503\n",
      "Iteration #270 loss: 0.1486005038022995\n",
      "Iteration #280 loss: 0.04861380159854889\n",
      "Iteration #290 loss: 0.11820843815803528\n",
      "Iteration #300 loss: 0.15109385550022125\n",
      "Iteration #310 loss: 0.09599786251783371\n",
      "Iteration #320 loss: 0.10748932510614395\n",
      "Iteration #330 loss: 0.02463826723396778\n",
      "Iteration #340 loss: 0.08615097403526306\n",
      "Iteration #350 loss: 0.10910475999116898\n",
      "Iteration #360 loss: 0.1444849818944931\n",
      "Iteration #370 loss: 0.03321194276213646\n",
      "Iteration #380 loss: 0.055273912847042084\n",
      "Iteration #390 loss: 0.07296759635210037\n",
      "Iteration #400 loss: 0.08100011199712753\n",
      "Iteration #410 loss: 0.052067313343286514\n",
      "Iteration #420 loss: 0.0711638480424881\n",
      "Epoch #53 average loss: 0.09245487694980271\n",
      "Epoch #53 accuracy: 0.15707685846283073\n",
      "Iteration #10 loss: 0.057689208537340164\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(img\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[1;32m     28\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m---> 30\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/torchgpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchgpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchgpu/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[0;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchgpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchgpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchgpu/lib/python3.8/site-packages/torchvision/models/detection/rpn.py:361\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    359\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(features\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    360\u001b[0m objectness, pred_bbox_deltas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(features)\n\u001b[0;32m--> 361\u001b[0m anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manchor_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m num_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(anchors)\n\u001b[1;32m    364\u001b[0m num_anchors_per_level_shape_tensors \u001b[38;5;241m=\u001b[39m [o[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m objectness]\n",
      "File \u001b[0;32m~/anaconda3/envs/torchgpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchgpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchgpu/lib/python3.8/site-packages/torchvision/models/detection/anchor_utils.py:119\u001b[0m, in \u001b[0;36mAnchorGenerator.forward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    117\u001b[0m image_size \u001b[38;5;241m=\u001b[39m image_list\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    118\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m feature_maps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype, feature_maps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 119\u001b[0m strides \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    120\u001b[0m     [\n\u001b[1;32m    121\u001b[0m         torch\u001b[38;5;241m.\u001b[39mempty((), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfill_(image_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m g[\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m    122\u001b[0m         torch\u001b[38;5;241m.\u001b[39mempty((), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfill_(image_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m g[\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m    123\u001b[0m     ]\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grid_sizes\n\u001b[1;32m    125\u001b[0m ]\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_cell_anchors(dtype, device)\n\u001b[1;32m    127\u001b[0m anchors_over_all_feature_maps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_anchors(grid_sizes, strides)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchgpu/lib/python3.8/site-packages/torchvision/models/detection/anchor_utils.py:122\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    117\u001b[0m image_size \u001b[38;5;241m=\u001b[39m image_list\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    118\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m feature_maps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype, feature_maps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    119\u001b[0m strides \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    120\u001b[0m     [\n\u001b[1;32m    121\u001b[0m         torch\u001b[38;5;241m.\u001b[39mempty((), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfill_(image_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m g[\u001b[38;5;241m0\u001b[39m]),\n\u001b[0;32m--> 122\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfill_(image_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m g[\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m    123\u001b[0m     ]\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grid_sizes\n\u001b[1;32m    125\u001b[0m ]\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_cell_anchors(dtype, device)\n\u001b[1;32m    127\u001b[0m anchors_over_all_feature_maps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_anchors(grid_sizes, strides)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torchvision.models.detection as models\n",
    "import torchvision.models.detection.faster_rcnn as fasterrcnn\n",
    "import torch\n",
    "\n",
    "# 加载预训练的 Faster R-CNN 模型\n",
    "model = models.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# 替换分类器的头部以适应新的类别数（假设只有一个类别加背景）\n",
    "num_classes = 7  # 1 class + background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = fasterrcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# 将模型移动到正确的设备\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 选择优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# 训练循环\n",
    "num_epochs = 100  # 假设我们训练10个epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    for images, targets in train_loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        num_batches += 1\n",
    "        if num_batches % 10 == 0:  # 每10次迭代打印一次训练损失\n",
    "            print(f\"Iteration #{num_batches} loss: {losses.item()}\")\n",
    "\n",
    "    # 计算整个 epoch 的平均损失\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch #{epoch+1} average loss: {avg_loss}\")\n",
    "\n",
    "    # 在每个epoch结束后评估模型\n",
    "    accuracy = evaluate(model, test_loader, device)\n",
    "    print(f\"Epoch #{epoch+1} accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
